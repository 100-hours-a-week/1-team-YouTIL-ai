{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a3115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri011228/ai2-server/deeplearning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:34:36 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:34:38,199\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:34:47 [config.py:689] This model supports multiple tasks: {'score', 'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-14 00:34:47 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-14 00:34:49 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/home/yuri011228/ai2-server/models/mistral-7b', speculative_config=None, tokenizer='/home/yuri011228/ai2-server/models/mistral-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/yuri011228/ai2-server/models/mistral-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-14 00:34:50 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1d9bf86f90>\n",
      "INFO 05-14 00:34:50 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-14 00:34:50 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-14 00:34:50 [gpu_model_runner.py:1276] Starting to load model /home/yuri011228/ai2-server/models/mistral-7b...\n",
      "WARNING 05-14 00:34:50 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.30s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.40s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.48s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.45s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:34:55 [loader.py:458] Loading weights took 4.65 seconds\n",
      "INFO 05-14 00:34:55 [gpu_model_runner.py:1291] Model loading took 13.4967 GiB and 4.848054 seconds\n",
      "INFO 05-14 00:35:07 [backends.py:416] Using cache directory: /home/yuri011228/.cache/vllm/torch_compile_cache/8f3f217165/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-14 00:35:07 [backends.py:426] Dynamo bytecode transform time: 11.19 s\n",
      "INFO 05-14 00:35:08 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 05-14 00:35:16 [monitor.py:33] torch.compile takes 11.19 s in total\n",
      "INFO 05-14 00:35:19 [kv_cache_utils.py:634] GPU KV cache size: 49,296 tokens\n",
      "INFO 05-14 00:35:19 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 12.04x\n",
      "INFO 05-14 00:35:54 [gpu_model_runner.py:1626] Graph capturing finished in 35 secs, took 0.51 GiB\n",
      "INFO 05-14 00:35:54 [core.py:163] init engine (profile, create kv cache, warmup model) took 58.21 seconds\n",
      "INFO 05-14 00:35:54 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "from qdrant_client import QdrantClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "base_llm = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=base_llm,\n",
    "    tensor_parallel_size=1, # GPU 개수\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_num_seqs = 100, # 동시에 받을 수 있는 요청 개수\n",
    "    max_model_len=4096, # input + output 토큰 길이\n",
    "    max_num_batched_tokens=8192) # 한 batch 당 토근 길이 \n",
    "\n",
    "llm = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 확인용\n",
    "# os.environ[\"LANGSMITH_TRACING\"]  \n",
    "# print(os.getenv(\"QDRANT_HOST\"))\n",
    "# print(os.getenv(\"QDRANT_PORT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29ee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = {\n",
    "  \"email\": \"ConconDev\",\n",
    "  \"date\": \"2024-09-06\",\n",
    "  \"level\": 1,\n",
    "  \"title\": \"알람 구독 서비스 개선: SseEmitter 활용 및 사용자별 Emitter 관리\",\n",
    "  \"keywords\": [\n",
    "    \"SSE\",\n",
    "    \"SseEmitter\",\n",
    "    \"알람 구독\",\n",
    "    \"사용자별 관리\"\n",
    "  ],\n",
    "  \"til\": \"# 알람 구독 서비스 개선: SseEmitter 활용 및 사용자별 Emitter 관리\\n\\n## 1. 오늘 배운 내용\\n\\n오늘 저는 알람 구독 서비스를 개선하기 위해 `SseEmitter`를 활용하고, 각 사용자별로 `SseEmitter`를 저장하고 관리하는 기능을 구현했습니다.  `SseEmitter`는 서버에서 클라이언트로 실시간 데이터를 스트리밍하는 데 유용한 API입니다.\\n\\n## 2. 개념 정리\\n\\n*   **SSE (Server-Sent Events):** 서버에서 클라이언트로 단방향 통신을 가능하게 하는 웹 기술입니다. 서버가 새로운 이벤트 발생 시 클라이언트에게 자동으로 업데이트를 전송합니다.\\n*   **SseEmitter:** SSE 통신을 위한 객체입니다.  데이터를 발행하거나, 연결을 종료하거나, 오류를 처리하는 등의 기능을 제공합니다.\\n*   **ConcurrentHashMap:** 여러 스레드에서 동시에 접근해도 안전하게 데이터를 저장하고 검색할 수 있는 해시맵입니다.  여기서는 사용자 ID를 키로 하고 `SseEmitter`를 값으로 저장하는 데 사용되었습니다.\\n\\n## 3. 해당 개념이 필요한 이유\\n\\n기존 알람 구독 방식은 클라이언트가 주기적으로 서버에 요청을 보내는 방식으로, 서버 부하가 심하고 효율성이 떨어졌습니다. `SseEmitter`를 사용하면 서버는 새로운 알람이 발생했을 때만 클라이언트에 데이터를 전송하므로, 서버 자원을 절약하고 실시간성을 높일 수 있습니다. 또한, 사용자별로 `SseEmitter`를 관리함으로써, 특정 사용자에 대한 알람만 전송하도록 할 수 있어 더욱 효율적인 알람 구독 시스템을 구축할 수 있습니다.\\n\\n## 4. 개념을 활용하는 방법\\n\\n1.  `SseEmitter` 객체를 생성하고 초기화합니다.\\n2.  사용자 ID를 키로, `SseEmitter` 객체를 값으로 `ConcurrentHashMap`에 저장합니다.\\n3.  클라이언트에서 SSE 연결을 설정하고, 서버로부터 데이터를 수신합니다.\\n4.  사용자가 알람 구독/취소 요청을 하면, 해당 사용자의 `SseEmitter`를 업데이트하거나 삭제합니다.\\n\\n## 5. 문제 해결 과정\\n\\n*   처음에는 `SseEmitter`의 생명주기를 제대로 관리하지 못하여 메모리 누수가 발생했습니다.  `SseEmitter` 객체의 `close()` 메서드를 호출하여 리소스 누수를 방지했습니다.\\n*   사용자 ID 중복 문제를 해결하기 위해 사용자 ID를 키로 사용하는 `ConcurrentHashMap`을 사용했습니다.\\n*   클라이언트에서 SSE 연결을 안정적으로 유지하기 위해 에러 핸들링 로직을 추가했습니다.\\n\\n## 6. 하루 회고\\n\\n오늘은 알람 구독 서비스의 성능 개선을 위해 중요한 기술인 `SseEmitter`를 익히고 적용하는 시간을 가졌습니다.  `SseEmitter`의 동작 원리를 이해하고, 실제 서비스에 적용하면서 많은 어려움을 겪었지만, 결국 성공적으로 구현할 수 있었습니다. 앞으로는 `SseEmitter`를 더 깊이 이해하고, 다양한 응용 분야에 적용해보고 싶습니다.\\n\\n## 7. 전체적으로 개조식 문장 구성\\n\\n*   **목표:** 알람 구독 서비스의 실시간성 및 효율성 향상\\n*   **핵심 기술:** SSE, SseEmitter, ConcurrentHashMap\\n*   **구현 단계:**\\n    *   `SseEmitter` 객체 생성 및 초기화\\n    *   사용자별 `SseEmitter` 저장 및 관리 (`ConcurrentHashMap`) \\n    *   SSE 연결 설정 및 데이터 수신\\n    *   알람 구독/취소 요청 처리\\n    *   메모리 누수 방지 및 에러 핸들링\\n\\n\"\n",
    "}\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "You are a technical interviewer AI.\n",
    "\n",
    "Your task is to generate exactly one interview question written in Korean, based on the following inputs:\n",
    "\n",
    "- Level: {level}\n",
    "- User TIL: {til}\n",
    "- Reference documents: {retrieved}\n",
    "\n",
    "## Output Instructions (strict):\n",
    "- Your response must be a **single complete sentence** in **Korean**.\n",
    "- The sentence must be a **clear interview-style question**, using natural question forms such as:\n",
    "  “~입니까?”, “~있나요?”, “~설명해주세요”, “~어떻게 되나요?”, “~어떻게 생각하시나요?” etc.\n",
    "- It must **not** be a declarative or answer-style sentence (e.g., ending with “~입니다”, “~합니다” ❌)\n",
    "\n",
    "- ⚠️ Do NOT include any of the following:\n",
    "  - English words or explanations\n",
    "  - Headings, notes, or comments\n",
    "  - Labels such as “Question:”, “Answer:”, “Note:”, or anything similar\n",
    "  - Markdown symbols (e.g., **, ``, →, #, ##)\n",
    "  - Emojis, quotation marks, parentheses, or line breaks\n",
    "\n",
    "- Only write the Korean question sentence. Nothing else.\n",
    "\n",
    "## Depth Control:\n",
    "- Level 1: Ask about deep technical understanding and implementation logic\n",
    "- Level 2: Ask about conceptual understanding\n",
    "- Level 3: Ask about basic theoretical concepts\n",
    "\n",
    "Respond with only one clean Korean question sentence. No explanations, no formatting, no extra text.\n",
    "\n",
    "question:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "You are an AI assistant that answers a technical interview question based on the user's learning record.\n",
    "\n",
    "Here is the input:\n",
    "- Question: {question}\n",
    "- User TIL: {til}\n",
    "- Level: {level}\n",
    "- Reference documents: {context}\n",
    "\n",
    "Do not repeat the question.  \n",
    "Generate **only one answer**, in **Korean**, based on the above information.  \n",
    "Keep your answer **concise**, **clear**, and **free of unnecessary symbols** or decorations.\n",
    "\n",
    "Just provide the answer in plain Korean. No introduction or explanation is needed.\n",
    "\n",
    "answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt3 = \"\"\"\n",
    "You are an AI assistant that summarizes a technical interview question and its answer into a short, meaningful Korean title.\n",
    "\n",
    "Your goal is to create a clear and specific title that would fit well in a developer document or a technical spec.\n",
    "\n",
    "Requirements:\n",
    "- The title must be written in **Korean**\n",
    "- The title must be **15 characters or fewer**\n",
    "- Do NOT include any quotation marks, punctuation, or extra lines\n",
    "- Write only the final title\n",
    "\n",
    "Example:\n",
    "Q: REST API란 무엇인가요?  \n",
    "A: REST API는 HTTP 프로토콜을 기반으로 자원을 URI로 표현하고, CRUD를 HTTP 메서드로 수행하는 아키텍처입니다.  \n",
    "title: REST API 개념 및 구성 요소\n",
    "\n",
    "Now summarize the following Q&A in the same way.\n",
    "\n",
    "{qacombined}\n",
    "\n",
    "title:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a71449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class ContentState(BaseModel):\n",
    "    question: str\n",
    "    answer: str \n",
    "\n",
    "class QAState(BaseModel):\n",
    "    email: str\n",
    "    date: str\n",
    "    level: int\n",
    "    title: str\n",
    "    keywords: List[str]\n",
    "    til: str\n",
    "\n",
    "    retrieved_texts: Optional[List[str]] = None\n",
    "    similarity_score: Optional[float] = None\n",
    "\n",
    "    question0: Optional[str] = None\n",
    "    question1: Optional[str] = None\n",
    "    question2: Optional[str] = None\n",
    "\n",
    "    question: Optional[str] = None\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "    content0: Optional[ContentState] = None\n",
    "    content1: Optional[ContentState] = None\n",
    "    content2: Optional[ContentState] = None\n",
    "\n",
    "    #output\n",
    "    content: Optional[List[ContentState]] = None\n",
    "    summary: Optional[str] = None\n",
    "\n",
    "\n",
    "qa_input = QAState(\n",
    "    email=dummy['email'],\n",
    "    date=dummy['date'],\n",
    "    level=dummy['level'],\n",
    "    title=dummy['title'],\n",
    "    keywords=dummy['keywords'],\n",
    "    til=dummy['til']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee0027",
   "metadata": {},
   "source": [
    "### 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d032c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from uuid import uuid4\n",
    "from vllm import SamplingParams\n",
    "import re\n",
    "\n",
    "class QAFlow:\n",
    "    def __init__(self, llm, qdrant, prompt1, prompt2, prompt3, max_nodes=3):\n",
    "        self.llm = llm\n",
    "        self.qdrant = qdrant\n",
    "        self.prompt1 = prompt1\n",
    "        self.prompt2 = prompt2\n",
    "        self.prompt3 = prompt3\n",
    "        self.max_nodes = max_nodes\n",
    "        self.embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "    def embed_text(self, text: str) -> list[float]:\n",
    "        return self.embedding_model.encode(text).tolist()\n",
    "\n",
    "    async def retriever_node(self, state: QAState) -> dict:\n",
    "        query = state.title + \" \" + \" \".join(state.keywords)\n",
    "        query_vector = self.embed_text(query)\n",
    "\n",
    "        collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "        best_score = 0.0\n",
    "        retrieved_texts = []\n",
    "\n",
    "        for col in collection_names:\n",
    "            results = self.qdrant.search(\n",
    "                collection_name=col,\n",
    "                query_vector=query_vector,\n",
    "                limit=3,\n",
    "                with_payload=True\n",
    "            )\n",
    "            if results and results[0].score > best_score:\n",
    "                best_score = results[0].score\n",
    "                retrieved_texts = [r.payload[\"text\"] for r in results if \"text\" in r.payload]\n",
    "\n",
    "        return {\n",
    "            \"similarity_score\": best_score,\n",
    "            \"retrieved_texts\": retrieved_texts\n",
    "        }\n",
    "    \n",
    "    # 후처리 추가 \n",
    "    def clean_korean_question(self, text: str) -> str:\n",
    "\n",
    "        text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "\n",
    "        # 라벨 및 마크다운 제거\n",
    "        text = re.sub(r'\\*\\*?(Question|Answer|Note|Level).*?\\*\\*?', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'(Question|Answer|Level)\\s*[:：]*', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'^#+\\s*', '', text)\n",
    "\n",
    "        # 문장 맨 앞 하이픈/번호 제거\n",
    "        text = re.sub(r'^[-•\\s]+\\d*\\s*', '', text)\n",
    "\n",
    "        # 괄호 level (띄어쓰기 포함) 제거\n",
    "        text = re.sub(r'\\(\\s*\\d+\\s*\\)', '', text)\n",
    "\n",
    "        # 기타 특수문자 제거\n",
    "        text = text.replace(\"`\", \"\").replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "        text = text.replace(\"👉\", \"\").replace(\"→\", \"\").strip()\n",
    "        text = text.strip().strip('\"“”')\n",
    "\n",
    "        # 줄 단위로 나누기\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "        question_endings = [\"?\", \"요.\", \"습니까\", \"설명해주세요\", \"어떻게\", \"무엇\", \n",
    "                            \"설명하시오\", \"구현하시오\", \"알려주세요\", \"어떤가요\", \"왜 그런가요\"]\n",
    "        \n",
    "        # 완결된 질문형 문장만 탐색\n",
    "        for line in lines:\n",
    "            if any(ending in line for ending in question_endings):\n",
    "                return line\n",
    "\n",
    "        return lines[0] if lines else \"\"\n",
    "\n",
    "\n",
    "    def generate_question_node(self, node_id: int):\n",
    "        async def question_node(state: QAState) -> dict:\n",
    "            retrieved = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "            \n",
    "            prompt1 = self.prompt1.format(\n",
    "                til=state.til,\n",
    "                level=state.level,\n",
    "                retrieved=retrieved\n",
    "            )\n",
    "\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.7,\n",
    "                max_tokens=128,\n",
    "                stop_token_ids=[2]\n",
    "            )\n",
    "\n",
    "            request_id = str(uuid4())\n",
    "            final_text = \"\"\n",
    "\n",
    "            async for output in self.llm.generate(\n",
    "                prompt=prompt1,\n",
    "                sampling_params=sampling_params,\n",
    "                request_id=request_id\n",
    "            ):\n",
    "                final_text = output.outputs[0].text.strip()\n",
    "\n",
    "            cleaned_question = self.clean_korean_question(final_text)\n",
    "\n",
    "            # print(node_id, final_text)\n",
    "            # print(node_id, cleaned_question)\n",
    "\n",
    "            return {f\"question{node_id}\": cleaned_question}\n",
    "\n",
    "        return question_node\n",
    "\n",
    "    def generate_answer_node(self, node_id: int):\n",
    "        async def answer_node(state: QAState) -> dict:\n",
    "            question = getattr(state, f\"question{node_id}\", None)\n",
    "            if not question:\n",
    "                raise ValueError(f\"질문 {node_id}가 없습니다.\")\n",
    "\n",
    "            context = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "\n",
    "            prompt2 = self.prompt2.format(\n",
    "                question=question,\n",
    "                til=state.til,\n",
    "                level=state.level,\n",
    "                context=context\n",
    "            )\n",
    "\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.7,\n",
    "                max_tokens=256,\n",
    "                stop_token_ids=[2]\n",
    "            )\n",
    "\n",
    "            request_id = str(uuid4())\n",
    "            final_text = \"\"\n",
    "\n",
    "            async for output in self.llm.generate(\n",
    "                prompt=prompt2,\n",
    "                sampling_params=sampling_params,\n",
    "                request_id=request_id\n",
    "            ):\n",
    "                final_text = output.outputs[0].text.strip()\n",
    "\n",
    "            return {\n",
    "                #\"content\": [ContentState(question=question, answer=final_text)]\n",
    "                f\"content{node_id}\": ContentState(\n",
    "                    question=question,\n",
    "                    answer=final_text)\n",
    "            }\n",
    "\n",
    "        return answer_node\n",
    "\n",
    "    async def summary_node(self, state: QAState) -> dict:\n",
    "        merged = []\n",
    "        for i in range(3):\n",
    "            item = getattr(state, f\"content{i}\", None)\n",
    "            if item:\n",
    "                merged.append(item)\n",
    "\n",
    "        qacombined = \"\\n\".join(\n",
    "            f\"Q: {item.question}\\nA: {item.answer}\" for item in merged\n",
    "        )\n",
    "\n",
    "        prompt3 = self.prompt3.format(\n",
    "            qacombined = qacombined\n",
    "        )\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.3,\n",
    "            max_tokens=32,\n",
    "            stop_token_ids=[2]\n",
    "        )\n",
    "\n",
    "        request_id = str(uuid4())\n",
    "        final_text = \"\"\n",
    "\n",
    "        async for output in self.llm.generate(\n",
    "            prompt=prompt3,\n",
    "            sampling_params=sampling_params,\n",
    "            request_id=request_id\n",
    "        ):\n",
    "            final_text = output.outputs[0].text.strip()\n",
    "\n",
    "        return {\n",
    "            \"summary\": final_text,\n",
    "            \"content\": merged\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        workflow = StateGraph(QAState)\n",
    "        workflow.set_entry_point(\"retriever\")\n",
    "        workflow.add_node(\"retriever\", self.retriever_node)\n",
    "\n",
    "        for i in range(self.max_nodes):\n",
    "            workflow.add_node(f\"que{i}\", self.generate_question_node(i))\n",
    "            workflow.add_node(f\"ans{i}\", self.generate_answer_node(i))\n",
    "\n",
    "            workflow.add_edge(\"retriever\", f\"que{i}\")\n",
    "            workflow.add_edge(f\"que{i}\", f\"ans{i}\")\n",
    "            workflow.add_edge(f\"ans{i}\", \"summary_generate\")\n",
    "\n",
    "        workflow.add_node(\"summary_generate\", self.summary_node)\n",
    "        workflow.set_finish_point(\"summary_generate\")\n",
    "\n",
    "        return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a844486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:36:36 [async_llm.py:228] Added request 1da07a71-2f2a-40c5-a209-a65cdbc79c1a.\n",
      "INFO 05-14 00:36:36 [async_llm.py:228] Added request 8b6871dc-2652-4781-ab3d-8a7c8b961a11.\n",
      "INFO 05-14 00:36:36 [async_llm.py:228] Added request dc997c6d-a70b-41ce-a233-2b744d6ff28f.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13410/2998371373.py:29: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.qdrant.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:36:43 [async_llm.py:228] Added request 20c84df0-f8dd-430d-be02-63f208e9807d.\n",
      "INFO 05-14 00:36:43 [async_llm.py:228] Added request 47b8886c-186c-4dd4-af8d-5cf525aae9e0.\n",
      "INFO 05-14 00:36:43 [async_llm.py:228] Added request 259541c8-e817-48fa-ba6a-3e18e360dca5.\n",
      "INFO 05-14 00:37:01 [async_llm.py:228] Added request fa34bed2-9509-43b8-abe9-b805656afcca.\n",
      "SSE 알람 서비스: 사용자별 SseEmitter 관리용 ConcurrentHashMap\n",
      "\n",
      "title:\n",
      "\n",
      "S\n",
      "어떤 방식으로 SseEmitter 객체를 생성하고 사용자별 저장 및 관리하는 ConcurrentHashMap을 구성하여, 알람 구독 서비스를 개선하는데 도움이 되는지 설명해주세요?\n",
      "SSE(서버 전송 이벤트)를 활용하여, 알람 구독 서비스를 개선하기 위해 사용자별 SseEmitter 객체를 생성하고 ConcurrentHashMap을 이용하여 관리하는 방법입니다. 이렇게 하면, 실시간 데이터 스트림을 지속적으로 전송하면서도 메모리 누수 문제를 해결하고, 특정 사용자에게만 알람을 전송하는 것이 가능해집니다. 웹브라우저는 최신 버전부터 모든 브라우저에서 SSE를 지원하고 있으며, 이를 활용하여 알람 구독 서비스의 실시간성과 효율성을 향상시킬 수 있습니다.\n",
      "\n",
      "사용자별 SseEmitter를 관리하는 방법에 대해 자세히 설명해주세요.\n",
      "SSE(Server-Sent Events)는 사용자가 HTTP 연결을 통해 서버로부터 자동 업데이트를 받는 서버 푸시 기술입니다. `SseEmitter`는 SSE를 구현하는 객체입니다. 각 사용자는 고유의 `SseEmitter`를 가지고 있어 실시간으로 데이터를 받을 수 있습니다. `ConcurrentHashMap`을 사용하여 사용자 ID를 키로, `SseEmitter`를 값으로 저장하여 사용자별로 `SseEmitter`를 관리할 수 있습니다. 이렇게 하면 효율적인 알람 서비스를 구현할 수 있습니다.\n",
      "\n",
      "사용자별 SseEmitter를 관리하는 ConcurrentHashMap에 대해, 해당 데이터 구조의 장점과 단점은 어떻게  différences: ？(ConcurrentHashMap의 사용자 별 SseEmitter 관리에 대한 데이터 구조의 장점과 단점은 어떻게 다르나요?)\n",
      "사용자별 SseEmitter를 관리하는 ConcurrentHashMap의 장점은 멀티스레드 환경에서도 안전하게 사용자의 SseEmitter 객체를 저장하고 관리할 수 있다는 점입니다. 여기서 ConcurrentHashMap는 여러 스레드가 동시에 동일한 데이터를 접근하여도 안전하게 데이터를 저장하고 검색할 수 있는 해시맵입니다. 또한, 사용자 ID를 고유한 키로 사용함으로써 사용자 중복 문제를 해결할 수 있습니다.\n",
      "\n",
      "사용자별 SseEmitter를 관리하는 ConcurrentHashMap의 단점은 동기화 비용이 높을 수 있습니다. ConcurrentHashMap은 일부 경우에서 동기화를 사용하\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_flow = QAFlow(llm=llm, qdrant=qdrant, prompt1=prompt1, prompt2=prompt2, prompt3=prompt3, max_nodes=3)\n",
    "graph = qa_flow.build_graph()\n",
    "result = await graph.ainvoke(qa_input)\n",
    "\n",
    "# 결과 출력\n",
    "print(result[\"summary\"])\n",
    "\n",
    "for c in result[\"content\"]:\n",
    "    print(c.question)\n",
    "    print(c.answer)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d8147c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f953f6f6fd0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e5edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tretriever(retriever)\n",
      "\tque0(que0)\n",
      "\tans0(ans0)\n",
      "\tque1(que1)\n",
      "\tans1(ans1)\n",
      "\tque2(que2)\n",
      "\tans2(ans2)\n",
      "\tsummary_generate(summary_generate)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> retriever;\n",
      "\tans0 --> summary_generate;\n",
      "\tans1 --> summary_generate;\n",
      "\tans2 --> summary_generate;\n",
      "\tque0 --> ans0;\n",
      "\tque1 --> ans1;\n",
      "\tque2 --> ans2;\n",
      "\tretriever --> que0;\n",
      "\tretriever --> que1;\n",
      "\tretriever --> que2;\n",
      "\tsummary_generate --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_flow = QAFlow(llm=llm, qdrant=qdrant, prompt_template=prompt1, max_nodes=3)\n",
    "graph = qa_flow.build_graph()\n",
    "\n",
    "# 시각화 코드\n",
    "print(graph.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5177459",
   "metadata": {},
   "source": [
    "### 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e018ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity_score': 0.52782375, 'retrieved_texts': ['서버 전송 이벤트(Server-sent events, SSE)는 클라이언트가 HTTP 연결을 통해 서버로부터 자동 업데이트를 수신할 수 있도록 하는 서버 푸시 기술이며, 초기 클라이언트 연결이 설정된 후 서버가 클라이언트를 향한 데이터 전송을 시작하는 방법을 설명한다. 이는 일반적으로 브라우저 클라이언트에 메시지 업데이트 또는 지속적인 데이터 스트림을 보내는 데 사용되며 클라이언트가 이벤트 스트림을 수신하기 위해 특정 URL을 요청하는 EventSource라는 자바스크립트 API를 통해 기본 브라우저 간 스트리밍을 향상시키도록 설계되었다. EventSource API는 WHATWG에 의해 HTML Living Standard의 일부로 표준화되었다. SSE의 미디어 유형은 text/event-stream이다. 파이어폭스 6+, 구글 크롬 6+, 오페라 11.5+, 사파리 5+, 마이크로소프트 엣지 79+ 등 모든 최신 브라우저는 서버에서 전송되는 이벤트를 지원한다.', '푸시 기법 기술은 풀 기법으로 명명된 웹브라우저와 비교할 수 있다. 현재 인터넷에서 사용되고 있는 웹브라우저는 사용할 때에 사용자가 항상 자신이 갖고자 하는 정보를 소유한 서버에게 정보를 요청하며 웹브라우저의 사용과 목적지에 대한 최종결정 역시 사용자가 결정할 수 있다. 반면 푸시 기법은 사용자가 일일이 요청하지 않아도 사용자에게 자동으로 뉴스나 사용자가 원하는 특별한 정보, 예를 들면 증권시장의 주기적인 정보 같은 것을 제공한다. 이 두 가지 기법의 차이점은 정보의 흐름을 누가 통제하느냐 하는 점에 있다고 할 수 있다. 풀 기법 하에서는 사용자 (즉 소비자)들이 정보 취득 및 정보의 접촉을 마음대로 통제할 수 있으나, 푸시 기법 하에서는 정보를 전달하는 쪽에서(즉 광고주)정보의 흐름을 직접 통제할 수 있게 된다.', '푸시 기법의 가장 큰 이점은 역시 정보의 맞춤화(Customization)가 가능하다는 점에 있다. 즉 사용자가 이미 등록되어 있기 때문에 등록된 사용자 정보에 의해 타겟을 정확히 선정할 수 있는 것이다.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3820/2475243331.py:23: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant.search(\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from vllm import SamplingParams\n",
    "from uuid import uuid4\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "def embed_text(text: str) -> list[float]:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "async def retriever_node(state: QAState) -> dict:\n",
    "    query = state.title + \" \" + \" \".join(state.keywords)\n",
    "    query_vector = embed_text(query)\n",
    "\n",
    "    collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "    best_score = 0.0\n",
    "\n",
    "    for col in collection_names:\n",
    "        results = qdrant.search(\n",
    "            collection_name=col,\n",
    "            query_vector=query_vector,\n",
    "            limit=3,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        if results and results[0].score > best_score:\n",
    "            best_score = results[0].score\n",
    "            retrieved_texts = [h.payload[\"text\"] for h in results if \"text\" in h.payload]\n",
    "\n",
    "    return {\n",
    "        \"similarity_score\": best_score,\n",
    "        \"retrieved_texts\": retrieved_texts\n",
    "    }\n",
    "\n",
    "result = await retriever_node(qa_input)\n",
    "qa_input.retrieved_texts = result[\"retrieved_texts\"]\n",
    "qa_input.similarity_score = result[\"similarity_score\"]\n",
    "print(result)\n",
    "\n",
    "async def question_node(state: QAState) -> dict:\n",
    "\n",
    "    retrieved = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "\n",
    "    prompt = prompt1.format(\n",
    "        til=state.til,\n",
    "        level=state.level,\n",
    "        retrieved=retrieved\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[2], \n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"question\": final_text\n",
    "    }\n",
    "\n",
    "# q = await question_node(qa_input)\n",
    "# qa_input.question = q[\"question\"]\n",
    "# print(q[\"question\"])\n",
    "\n",
    "async def answer_node(state: QAState) -> dict:\n",
    "    if not state.question:\n",
    "        raise ValueError(\"질문이 없습니다. 먼저 generate_node를 통해 질문을 생성해야 합니다.\")\n",
    "\n",
    "    context = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "    \n",
    "    prompt =f\"\"\"\n",
    "    당신은 사용자의 기술 학습 기록을 바탕으로, 하나의 기술 면접 답변을 생성하는 AI입니다.\n",
    "\n",
    "    아래 정보를 참고하여,\n",
    "    질문: {state.question}\n",
    "    사용자 TIL: {state.til}\n",
    "    level: {state.level}\n",
    "    - 참고 문서 {context}\n",
    "\n",
    "    ※ level에 따라 질문 수준을 조절해서 질문에 대한 답변 \"\"1개\"\"를 작성해주세요:\n",
    "    - level \"1\": 깊은 기술 이해와 실무 경험 기반 질문\n",
    "    - level \"2\": 개념적 이해를 묻는 질문\n",
    "    - level \"3\": 기본 개념을 묻는 질문\n",
    "\n",
    "    반드시 하나의 답변만 만들고 **한국어**로 작성해주세요.\n",
    "    아래와 같은 형식으로 출력하되, 답변에 상관없는 기호나 문자는 빼주세요.\n",
    "    답변: \" \"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[2],\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"answer\": final_text,\n",
    "        \"content\": [\n",
    "            ContentState(\n",
    "                question=state.question,\n",
    "                answer=final_text\n",
    "            )\n",
    "        ]        \n",
    "    }\n",
    "\n",
    "# a = await answer_node(qa_input)\n",
    "# qa_input.answer = a[\"answer\"]\n",
    "# qa_input.content = a[\"content\"]\n",
    "\n",
    "# print(\"🧾 질문:\", qa_input.content[0].question)\n",
    "# print(\"💬 답변:\", qa_input.content[0].answer)\n",
    "\n",
    "async def summary_node(state: QAState) -> dict:\n",
    "    content_items = state.content or []\n",
    "\n",
    "    combined = \"\\n\".join(\n",
    "        f\"Q: {item.question}\\nA: {item.answer}\" for item in content_items\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    다음은 면접 질문과 그에 대한 답변입니다. 질문과 답변의 내용을 보고 핵심 주제를 한 줄로 정리해주세요.  \n",
    "    이 제목은 개발 문서나 기능 설명서에서 쓸 수 있을 정도로 간결하고 구체적이어야 합니다.\n",
    "\n",
    "    예시:\n",
    "    Q: REST API란 무엇인가요?  \n",
    "    A: REST API는 HTTP 프로토콜을 기반으로 자원을 URI로 표현하고, CRUD를 HTTP 메서드로 수행하는 아키텍처입니다.  \n",
    "    → 제목: REST API 개념 및 구성 요소\n",
    "\n",
    "    다음 질문과 답변을 참고해서 위와 같은 형식으로 **15글자** 이내로 제목을 정리해주세요.\n",
    "\n",
    "    {combined}\n",
    "\n",
    "    → 제목:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.3,\n",
    "        max_tokens=32,\n",
    "        stop_token_ids=[2]\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"summary\": final_text \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43299b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 09:49:26 [async_llm.py:228] Added request 055832f9-c1b2-4b14-9a68-c35b26107530.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2433/2708729159.py:24: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 09:49:40 [async_llm.py:228] Added request c2bd692e-1409-43c3-898f-b9081ba77187.\n",
      "INFO 05-12 09:50:42 [async_llm.py:228] Added request d46e1a9a-c8dc-49e1-8d79-bce1fed8cbd3.\n"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(QAState)\n",
    "\n",
    "# 시작 지점 설정\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "\n",
    "# 노드 선언\n",
    "workflow.add_node(\"retriever\",retriever_node)\n",
    "\n",
    "workflow.add_node(\"question_generate\",question_node)\n",
    "workflow.add_node(\"answer_generate\",answer_node)\n",
    "\n",
    "workflow.add_node(\"summary_generate\",summary_node)\n",
    "\n",
    "workflow.add_edge(\"retriever\", \"question_generate\")\n",
    "workflow.add_edge(\"question_generate\", \"answer_generate\")\n",
    "workflow.add_edge(\"answer_generate\", \"summary_generate\")\n",
    "\n",
    "# 종료 지점 설정\n",
    "workflow.set_finish_point(\"summary_generate\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c4dcfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': 'ConconDev',\n",
       " 'date': '2024-09-06',\n",
       " 'level': 1,\n",
       " 'title': '알람 구독 서비스 개선: SseEmitter 활용 및 사용자별 Emitter 관리',\n",
       " 'keywords': ['SSE', 'SseEmitter', '알람 구독', '사용자별 관리'],\n",
       " 'til': '# 알람 구독 서비스 개선: SseEmitter 활용 및 사용자별 Emitter 관리\\n\\n## 1. 오늘 배운 내용\\n\\n오늘 저는 알람 구독 서비스를 개선하기 위해 `SseEmitter`를 활용하고, 각 사용자별로 `SseEmitter`를 저장하고 관리하는 기능을 구현했습니다.  `SseEmitter`는 서버에서 클라이언트로 실시간 데이터를 스트리밍하는 데 유용한 API입니다.\\n\\n## 2. 개념 정리\\n\\n*   **SSE (Server-Sent Events):** 서버에서 클라이언트로 단방향 통신을 가능하게 하는 웹 기술입니다. 서버가 새로운 이벤트 발생 시 클라이언트에게 자동으로 업데이트를 전송합니다.\\n*   **SseEmitter:** SSE 통신을 위한 객체입니다.  데이터를 발행하거나, 연결을 종료하거나, 오류를 처리하는 등의 기능을 제공합니다.\\n*   **ConcurrentHashMap:** 여러 스레드에서 동시에 접근해도 안전하게 데이터를 저장하고 검색할 수 있는 해시맵입니다.  여기서는 사용자 ID를 키로 하고 `SseEmitter`를 값으로 저장하는 데 사용되었습니다.\\n\\n## 3. 해당 개념이 필요한 이유\\n\\n기존 알람 구독 방식은 클라이언트가 주기적으로 서버에 요청을 보내는 방식으로, 서버 부하가 심하고 효율성이 떨어졌습니다. `SseEmitter`를 사용하면 서버는 새로운 알람이 발생했을 때만 클라이언트에 데이터를 전송하므로, 서버 자원을 절약하고 실시간성을 높일 수 있습니다. 또한, 사용자별로 `SseEmitter`를 관리함으로써, 특정 사용자에 대한 알람만 전송하도록 할 수 있어 더욱 효율적인 알람 구독 시스템을 구축할 수 있습니다.\\n\\n## 4. 개념을 활용하는 방법\\n\\n1.  `SseEmitter` 객체를 생성하고 초기화합니다.\\n2.  사용자 ID를 키로, `SseEmitter` 객체를 값으로 `ConcurrentHashMap`에 저장합니다.\\n3.  클라이언트에서 SSE 연결을 설정하고, 서버로부터 데이터를 수신합니다.\\n4.  사용자가 알람 구독/취소 요청을 하면, 해당 사용자의 `SseEmitter`를 업데이트하거나 삭제합니다.\\n\\n## 5. 문제 해결 과정\\n\\n*   처음에는 `SseEmitter`의 생명주기를 제대로 관리하지 못하여 메모리 누수가 발생했습니다.  `SseEmitter` 객체의 `close()` 메서드를 호출하여 리소스 누수를 방지했습니다.\\n*   사용자 ID 중복 문제를 해결하기 위해 사용자 ID를 키로 사용하는 `ConcurrentHashMap`을 사용했습니다.\\n*   클라이언트에서 SSE 연결을 안정적으로 유지하기 위해 에러 핸들링 로직을 추가했습니다.\\n\\n## 6. 하루 회고\\n\\n오늘은 알람 구독 서비스의 성능 개선을 위해 중요한 기술인 `SseEmitter`를 익히고 적용하는 시간을 가졌습니다.  `SseEmitter`의 동작 원리를 이해하고, 실제 서비스에 적용하면서 많은 어려움을 겪었지만, 결국 성공적으로 구현할 수 있었습니다. 앞으로는 `SseEmitter`를 더 깊이 이해하고, 다양한 응용 분야에 적용해보고 싶습니다.\\n\\n## 7. 전체적으로 개조식 문장 구성\\n\\n*   **목표:** 알람 구독 서비스의 실시간성 및 효율성 향상\\n*   **핵심 기술:** SSE, SseEmitter, ConcurrentHashMap\\n*   **구현 단계:**\\n    *   `SseEmitter` 객체 생성 및 초기화\\n    *   사용자별 `SseEmitter` 저장 및 관리 (`ConcurrentHashMap`) \\n    *   SSE 연결 설정 및 데이터 수신\\n    *   알람 구독/취소 요청 처리\\n    *   메모리 누수 방지 및 에러 핸들링\\n\\n',\n",
       " 'retrieved_texts': ['서버 전송 이벤트(Server-sent events, SSE)는 클라이언트가 HTTP 연결을 통해 서버로부터 자동 업데이트를 수신할 수 있도록 하는 서버 푸시 기술이며, 초기 클라이언트 연결이 설정된 후 서버가 클라이언트를 향한 데이터 전송을 시작하는 방법을 설명한다. 이는 일반적으로 브라우저 클라이언트에 메시지 업데이트 또는 지속적인 데이터 스트림을 보내는 데 사용되며 클라이언트가 이벤트 스트림을 수신하기 위해 특정 URL을 요청하는 EventSource라는 자바스크립트 API를 통해 기본 브라우저 간 스트리밍을 향상시키도록 설계되었다. EventSource API는 WHATWG에 의해 HTML Living Standard의 일부로 표준화되었다. SSE의 미디어 유형은 text/event-stream이다. 파이어폭스 6+, 구글 크롬 6+, 오페라 11.5+, 사파리 5+, 마이크로소프트 엣지 79+ 등 모든 최신 브라우저는 서버에서 전송되는 이벤트를 지원한다.',\n",
       "  '푸시 기법 기술은 풀 기법으로 명명된 웹브라우저와 비교할 수 있다. 현재 인터넷에서 사용되고 있는 웹브라우저는 사용할 때에 사용자가 항상 자신이 갖고자 하는 정보를 소유한 서버에게 정보를 요청하며 웹브라우저의 사용과 목적지에 대한 최종결정 역시 사용자가 결정할 수 있다. 반면 푸시 기법은 사용자가 일일이 요청하지 않아도 사용자에게 자동으로 뉴스나 사용자가 원하는 특별한 정보, 예를 들면 증권시장의 주기적인 정보 같은 것을 제공한다. 이 두 가지 기법의 차이점은 정보의 흐름을 누가 통제하느냐 하는 점에 있다고 할 수 있다. 풀 기법 하에서는 사용자 (즉 소비자)들이 정보 취득 및 정보의 접촉을 마음대로 통제할 수 있으나, 푸시 기법 하에서는 정보를 전달하는 쪽에서(즉 광고주)정보의 흐름을 직접 통제할 수 있게 된다.',\n",
       "  '푸시 기법의 가장 큰 이점은 역시 정보의 맞춤화(Customization)가 가능하다는 점에 있다. 즉 사용자가 이미 등록되어 있기 때문에 등록된 사용자 정보에 의해 타겟을 정확히 선정할 수 있는 것이다.'],\n",
       " 'similarity_score': 0.52782375,\n",
       " 'question0': '알람 구독 서비스를 개선하기 위해 `SseEmitter`와 `ConcurrentHashMap`을 사용하고 있습니다. 이러한 구성에는 어떤 장점이 있습니까? 이러한 구성은 무슨 상황에서 사용되는 것입니까?\\n즉, 알람 구독 서비스를 개선하기 위해 SSE와 `SseEmitter`, `ConcurrentHashMap`을 사용하는 이유와 사용 상황에 대해 질문하고 있습니다.',\n",
       " 'question1': '질문: \"알람 구독 서비스를 개선하기 위해 SseEmitter와 ConcurrentHashMap을 사용하고 있는데, 이러한 구성에는 어떤 장점이 있습니까?\"\\n\\n이 구성에는 알람 서비스의 성능을 향상시키고, 효율성을 높일 수 있는 장점이 있습니다. 이 구성은 클라이언트가 주기적으로 서버에 요청을 보내는 기존의 알람 구독 방식에 비해 서버 부하가 감소하고, 실시간성이 높아지는 이점이 있습니다. 또한 사용자별로 SseEmitter를 관리함으로써 특정 사용자에 대한 알람만 전송하도록 할 수 있어 더욱 효율적인 알람 구독 시스템을 구축할 수 있습니다.',\n",
       " 'question2': '질문: \"푸시 기법이 풀 기법과 어떤 차이점이 있습니까? 푸시 기법의 장점은 무엇입니까? 이러한 기술은 어떤 분야에서 자주 사용되고 있습니까?',\n",
       " 'content0': ContentState(question='알람 구독 서비스를 개선하기 위해 `SseEmitter`와 `ConcurrentHashMap`을 사용하고 있습니다. 이러한 구성에는 어떤 장점이 있습니까? 이러한 구성은 무슨 상황에서 사용되는 것입니까?\\n즉, 알람 구독 서비스를 개선하기 위해 SSE와 `SseEmitter`, `ConcurrentHashMap`을 사용하는 이유와 사용 상황에 대해 질문하고 있습니다.', answer='알람 구독 서비스에서 실시간성과 효율성을 높이기 위해 사용되는 기술 중 하나는 SSE(Server-Sent Events)입니다. SSE는 클라이언트가 HTTP 연결을 통해 서버로부터 자동 업데이트를 수신하는 서버 푸시 기술입니다. 이 기술을 활용하면 서버가 새로운 알람이 발생할 때마다 해당 알람을 클라이언트에 전송할 수 있습니다. SSE는 최신 브라우저에서 자동으로 지원하며, 이를 위해 EventSource라는 자바스크립트 API를 사용합니다.\\n\\n            이러한 기술을 구현하기 위해 `SseEmitter`와 `ConcurrentHashMap`을 사용할 수 있습니다. `SseEmitter`는 서버에서 클라이언트로 데이터를 스트리밍하는 데 사용되며, `ConcurrentHashMap`은 여러 스레드에서 동시에 접근해도 안전하게 데이터를 저장하고 검색할 수 있는 해시맵입니다. 사용자 ID를 키로, `SseEmitter`를 값으로 `ConcurrentHashMap`에 저장하여 각 사용자에 대한 `SseEmitter`를 관리할 수 있습니다. 이렇게 하면 서버는 새로운 알람이 발생하면 해당 사용자의 `SseEmitter`를 사용하여 해당 사용자에게만 알람을 전송할 수 있습니다.\\n\\n            이러한 구성에는 몇 가지 장점이 있습니다. 처음으로 알람 구독 서비스의 효율성을 높일 수 있습니다. 클라이언트는 정기적으로 서버에 요청하지 않아도 되므로, 서버 자원을 절약할 수 있습니다. 또한, 서버는 새로운 알람이 발생할 때마다 해당 알람을 전송하므로, 실시간성을 높일 수 있습니다. 또한, 사용자별로 `SseEmitter`를 관리함으로써 특정 사용자에 대한 알람만 전송하도록 할 수 있어 더욱 효율적인 알람 구독 시스템을 구축할 수 있습니다. 따라서, 알람 구독 서비스를 개선하기 위해 `SseEmitter`와 `ConcurrentHashMap`을 사용하는 것은 매우 유용한 접근 방식입니다.'),\n",
       " 'content1': ContentState(question='질문: \"알람 구독 서비스를 개선하기 위해 SseEmitter와 ConcurrentHashMap을 사용하고 있는데, 이러한 구성에는 어떤 장점이 있습니까?\"\\n\\n이 구성에는 알람 서비스의 성능을 향상시키고, 효율성을 높일 수 있는 장점이 있습니다. 이 구성은 클라이언트가 주기적으로 서버에 요청을 보내는 기존의 알람 구독 방식에 비해 서버 부하가 감소하고, 실시간성이 높아지는 이점이 있습니다. 또한 사용자별로 SseEmitter를 관리함으로써 특정 사용자에 대한 알람만 전송하도록 할 수 있어 더욱 효율적인 알람 구독 시스템을 구축할 수 있습니다.', answer='실시간 알림 서비스를 개선하기 위해 SseEmitter와 ConcurrentHashMap을 사용합니다. SseEmitter는 서버가 클라이언트에게 데이터를 전송하는 방법입니다. 이를 통해 효율성을 높일 수 있고, 서버 부하를 줄일 수 있습니다. ConcurrentHashMap은 여러 스레드에서 동시에 데이터를 저장하고 검색할 수 있는 해시맵입니다. 사용자별 SseEmitter를 저장하고 관리하여 특정 사용자에 대한 알림만 전송할 수 있습니다. 이를 통해 더욱 효율적인 알림 구독 서비스를 구현할 수 있습니다.'),\n",
       " 'content2': ContentState(question='질문: \"푸시 기법이 풀 기법과 어떤 차이점이 있습니까? 푸시 기법의 장점은 무엇입니까? 이러한 기술은 어떤 분야에서 자주 사용되고 있습니까?', answer='해당 기술은 풀 기법과의 차이점으로 인터넷에서 사용되고 있는 대부분의 웹 브라우저와 다르게 사용자가 요청하지 않아도 자동으로 사용자에게 뉴스나 특별한 정보를 제공하는 푸시 기법입니다. 이 기술의 가장 큰 장점은 정보의 맞춤화, 즉, 등록된 사용자 정보를 바탕으로 타겟을 정확히 선택하여 정보를 전달할 수 있다는 점입니다.'),\n",
       " 'content': [ContentState(question='알람 구독 서비스를 개선하기 위해 `SseEmitter`와 `ConcurrentHashMap`을 사용하고 있습니다. 이러한 구성에는 어떤 장점이 있습니까? 이러한 구성은 무슨 상황에서 사용되는 것입니까?\\n즉, 알람 구독 서비스를 개선하기 위해 SSE와 `SseEmitter`, `ConcurrentHashMap`을 사용하는 이유와 사용 상황에 대해 질문하고 있습니다.', answer='알람 구독 서비스에서 실시간성과 효율성을 높이기 위해 사용되는 기술 중 하나는 SSE(Server-Sent Events)입니다. SSE는 클라이언트가 HTTP 연결을 통해 서버로부터 자동 업데이트를 수신하는 서버 푸시 기술입니다. 이 기술을 활용하면 서버가 새로운 알람이 발생할 때마다 해당 알람을 클라이언트에 전송할 수 있습니다. SSE는 최신 브라우저에서 자동으로 지원하며, 이를 위해 EventSource라는 자바스크립트 API를 사용합니다.\\n\\n            이러한 기술을 구현하기 위해 `SseEmitter`와 `ConcurrentHashMap`을 사용할 수 있습니다. `SseEmitter`는 서버에서 클라이언트로 데이터를 스트리밍하는 데 사용되며, `ConcurrentHashMap`은 여러 스레드에서 동시에 접근해도 안전하게 데이터를 저장하고 검색할 수 있는 해시맵입니다. 사용자 ID를 키로, `SseEmitter`를 값으로 `ConcurrentHashMap`에 저장하여 각 사용자에 대한 `SseEmitter`를 관리할 수 있습니다. 이렇게 하면 서버는 새로운 알람이 발생하면 해당 사용자의 `SseEmitter`를 사용하여 해당 사용자에게만 알람을 전송할 수 있습니다.\\n\\n            이러한 구성에는 몇 가지 장점이 있습니다. 처음으로 알람 구독 서비스의 효율성을 높일 수 있습니다. 클라이언트는 정기적으로 서버에 요청하지 않아도 되므로, 서버 자원을 절약할 수 있습니다. 또한, 서버는 새로운 알람이 발생할 때마다 해당 알람을 전송하므로, 실시간성을 높일 수 있습니다. 또한, 사용자별로 `SseEmitter`를 관리함으로써 특정 사용자에 대한 알람만 전송하도록 할 수 있어 더욱 효율적인 알람 구독 시스템을 구축할 수 있습니다. 따라서, 알람 구독 서비스를 개선하기 위해 `SseEmitter`와 `ConcurrentHashMap`을 사용하는 것은 매우 유용한 접근 방식입니다.'),\n",
       "  ContentState(question='질문: \"알람 구독 서비스를 개선하기 위해 SseEmitter와 ConcurrentHashMap을 사용하고 있는데, 이러한 구성에는 어떤 장점이 있습니까?\"\\n\\n이 구성에는 알람 서비스의 성능을 향상시키고, 효율성을 높일 수 있는 장점이 있습니다. 이 구성은 클라이언트가 주기적으로 서버에 요청을 보내는 기존의 알람 구독 방식에 비해 서버 부하가 감소하고, 실시간성이 높아지는 이점이 있습니다. 또한 사용자별로 SseEmitter를 관리함으로써 특정 사용자에 대한 알람만 전송하도록 할 수 있어 더욱 효율적인 알람 구독 시스템을 구축할 수 있습니다.', answer='실시간 알림 서비스를 개선하기 위해 SseEmitter와 ConcurrentHashMap을 사용합니다. SseEmitter는 서버가 클라이언트에게 데이터를 전송하는 방법입니다. 이를 통해 효율성을 높일 수 있고, 서버 부하를 줄일 수 있습니다. ConcurrentHashMap은 여러 스레드에서 동시에 데이터를 저장하고 검색할 수 있는 해시맵입니다. 사용자별 SseEmitter를 저장하고 관리하여 특정 사용자에 대한 알림만 전송할 수 있습니다. 이를 통해 더욱 효율적인 알림 구독 서비스를 구현할 수 있습니다.'),\n",
       "  ContentState(question='질문: \"푸시 기법이 풀 기법과 어떤 차이점이 있습니까? 푸시 기법의 장점은 무엇입니까? 이러한 기술은 어떤 분야에서 자주 사용되고 있습니까?', answer='해당 기술은 풀 기법과의 차이점으로 인터넷에서 사용되고 있는 대부분의 웹 브라우저와 다르게 사용자가 요청하지 않아도 자동으로 사용자에게 뉴스나 특별한 정보를 제공하는 푸시 기법입니다. 이 기술의 가장 큰 장점은 정보의 맞춤화, 즉, 등록된 사용자 정보를 바탕으로 타겟을 정확히 선택하여 정보를 전달할 수 있다는 점입니다.')],\n",
       " 'summary': '푸시 기술 vs 풀 기술: 자동 정보 제공,'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e569f1",
   "metadata": {},
   "source": [
    "#### 레거시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3f43b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from uuid import uuid4\n",
    "from vllm import SamplingParams\n",
    "import os\n",
    "\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "def embed_text(text: str) -> list[float]:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "async def retriever_node(state: QAState) -> dict:\n",
    "    query = state.title + \" \" + \" \".join(state.keywords)\n",
    "    query_vector = embed_text(query)\n",
    "\n",
    "    collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "    best_score = 0.0\n",
    "    retrieved_texts: List[str] = []\n",
    "\n",
    "    for col in collection_names:\n",
    "        results = qdrant.search(\n",
    "            collection_name=col,\n",
    "            query_vector=query_vector,\n",
    "            limit=3,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        if results and results[0].score > best_score:\n",
    "            best_score = results[0].score\n",
    "            retrieved_texts = [h.payload[\"text\"] for h in results if \"text\" in h.payload]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"similarity_score\": best_score,\n",
    "        \"retrieved_texts\": retrieved_texts\n",
    "    }\n",
    "\n",
    "prom = \"\"\"\n",
    "당신은 사용자의 기술 학습 기록을 바탕으로, 기술 면접 질문을 생성하는 AI입니다.\n",
    "\n",
    "아래 정보를 참고하여,\n",
    "[TIL 본문] {til}\n",
    "[RAG 검색 결과] {text}\n",
    "[선택한 난이도] {level}\n",
    "\n",
    "※ level에 따라 질문 수준을 조절해서 면접 질문을 작성해주세요:\n",
    "- level \"1\": 깊은 기술 이해와 실무 경험 기반 질문\n",
    "- level \"2\": 개념적 이해를 묻는 질문\n",
    "- level \"3\": 기본 개념을 묻는 질문\n",
    "\n",
    "모든 질문과 답변은 반드시 **한국어**로 작성하세요.\n",
    "\n",
    "\"\"\"\n",
    "# 질문 생성 노드\n",
    "async def question0_node(state: QAState) -> Dict[str, Any]:\n",
    "    text = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "    prompt = prom.format(\n",
    "        til=state.til,\n",
    "        text=text,\n",
    "        level=state.level\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(prompt, sampling_params, request_id=request_id):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    # 질문만 추출\n",
    "    match = re.search(r\"질문[:：]\\s*(.+)\", final_text)\n",
    "    question = match.group(1).strip() if match else \"질문 생성 실패\"\n",
    "\n",
    "    return {\n",
    "        \"qa_0\": ContentState(question=question, answer=\"\")\n",
    "    }\n",
    "\n",
    "async def question1_node(state: QAState) -> Dict[str, Any]:\n",
    "    generated_q = f\"Q1: '{state.title}' 관련 두 번째 질문입니다.\"\n",
    "    return {\"qa_1\": ContentState(question=generated_q, answer=\"\")}\n",
    "\n",
    "async def question2_node(state: QAState) -> Dict[str, Any]:\n",
    "    generated_q = f\"Q2: '{state.title}' 관련 세 번째 질문입니다.\"\n",
    "    return {\"qa_2\": ContentState(question=generated_q, answer=\"\")}\n",
    "\n",
    "# 답변 생성 노드\n",
    "async def answer0_node(state: QAState) -> dict:\n",
    "    return {\"qa_0\": {\"question\": state.qa_0.question, \"answer\": \"답변0\"}}\n",
    "\n",
    "\n",
    "async def summary_node(state: QAState) -> dict:\n",
    "    # state.qa_0, qa_1, qa_2에 담긴 질문/답변을 모아서 최종 summary, content 생성\n",
    "    content = [state.qa_0]\n",
    "    summary = \"요약 생성 완료\"\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"content\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3570c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:430\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == requests.codes.ok:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/IPython/core/formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langgraph/pregel/__init__.py:634\u001b[39m, in \u001b[36mPregel._repr_mimebundle_\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_mimebundle_\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Mime bundle used by Jupyter to display the graph\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    633\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    635\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph.py:685\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    679\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    680\u001b[39m     curve_style=curve_style,\n\u001b[32m    681\u001b[39m     node_colors=node_colors,\n\u001b[32m    682\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    683\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    684\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:293\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    287\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    288\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    289\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    290\u001b[39m         )\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    301\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:462\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    457\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m             msg = (\n\u001b[32m    459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m             ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# This should not be reached, but just in case\u001b[39;00m\n\u001b[32m    465\u001b[39m msg = (\n\u001b[32m    466\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m ) + error_msg_suffix\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph at 0x7ff53e3d71d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7327878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f17a5eb6450>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프 정의 및 구성\n",
    "from langgraph.graph import END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "workflow = StateGraph(QAState)\n",
    "\n",
    "# 노드 선언\n",
    "workflow.add_node(\"retriever\", RunnableLambda(retriever_node).with_config({\"run_name\": \"retriever\"}))\n",
    "\n",
    "# 질문 생성 노드\n",
    "workflow.add_node(\"question0\", RunnableLambda(question0_node).with_config({\"run_name\": \"question0\"}))\n",
    "workflow.add_node(\"question1\", RunnableLambda(question1_node).with_config({\"run_name\": \"question1\"}))\n",
    "workflow.add_node(\"question2\", RunnableLambda(question2_node).with_config({\"run_name\": \"question2\"}))\n",
    "\n",
    "# 답변 생성 노드\n",
    "workflow.add_node(\"answer0\", RunnableLambda(answer0_node).with_config({\"run_name\": \"answer0\"}))\n",
    "workflow.add_node(\"answer1\", RunnableLambda(answer1_node).with_config({\"run_name\": \"answer1\"}))\n",
    "workflow.add_node(\"answer2\", RunnableLambda(answer2_node).with_config({\"run_name\": \"answer2\"}))\n",
    "\n",
    "# fallback + summary\n",
    "# workflow.add_node(\"fallback_generate\", RunnableLambda(fallback_generate_node).with_config({\"run_name\": \"fallback_generate\"}))\n",
    "workflow.add_node(\"summary_node\", RunnableLambda(summary_node).with_config({\"run_name\": \"summary_node\"}))\n",
    "\n",
    "# 조건부 분기 설정\n",
    "# def route_by_similarity(state: QAState) -> str:\n",
    "#     return \"generate_seq\" if state.similarity_score >= 0.5 else \"fallback_generate\"\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"retriever\",\n",
    "#     route_by_similarity,\n",
    "#     {\n",
    "#         \"generate_seq\": [\"question0\", \"question1\", \"question2\"],\n",
    "#         \"fallback_generate\": \"fallback_generate\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "workflow.add_edge(\"retriever\", \"question0\")\n",
    "workflow.add_edge(\"retriever\", \"question1\")\n",
    "workflow.add_edge(\"retriever\", \"question2\")\n",
    "\n",
    "\n",
    "# 각 질문 생성 → 답변 생성 연결\n",
    "workflow.add_edge(\"question0\", \"answer0\")\n",
    "workflow.add_edge(\"question1\", \"answer1\")\n",
    "workflow.add_edge(\"question2\", \"answer2\")\n",
    "\n",
    "# 각 답변 노드 → summary\n",
    "workflow.add_edge(\"answer0\", \"summary_node\")\n",
    "workflow.add_edge(\"answer1\", \"summary_node\")\n",
    "workflow.add_edge(\"answer2\", \"summary_node\")\n",
    "\n",
    "# fallback도 summary로\n",
    "# workflow.add_edge(\"fallback_generate\", \"summary_node\")\n",
    "\n",
    "# 시작, 종료 지점 설정\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "workflow.set_finish_point(\"summary_node\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc49f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "# result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이건 입력에 대한 질문입니다: LangGraph 연동 테스트'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from langsmith import traceable\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# @traceable(name=\"generate-question\")\n",
    "# def generate_question(input_text: str) -> str:\n",
    "#     return f\"이건 입력에 대한 질문입니다: {input_text}\"\n",
    "\n",
    "# generate_question(\"LangGraph 연동 테스트\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
