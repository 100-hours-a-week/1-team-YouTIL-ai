{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37a3115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 00:54:30 [config.py:689] This model supports multiple tasks: {'reward', 'score', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-12 00:54:30 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-12 00:54:31 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/home/yuri011228/ai2-server/models/mistral-7b', speculative_config=None, tokenizer='/home/yuri011228/ai2-server/models/mistral-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/yuri011228/ai2-server/models/mistral-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-12 00:54:32 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f5224a9a6d0>\n",
      "INFO 05-12 00:54:32 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-12 00:54:32 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-12 00:54:32 [gpu_model_runner.py:1276] Starting to load model /home/yuri011228/ai2-server/models/mistral-7b...\n",
      "WARNING 05-12 00:54:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.30s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.40s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.46s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 00:54:37 [loader.py:458] Loading weights took 4.68 seconds\n",
      "INFO 05-12 00:54:38 [gpu_model_runner.py:1291] Model loading took 13.4967 GiB and 4.870459 seconds\n",
      "INFO 05-12 00:54:48 [backends.py:416] Using cache directory: /home/yuri011228/.cache/vllm/torch_compile_cache/8f3f217165/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-12 00:54:48 [backends.py:426] Dynamo bytecode transform time: 10.70 s\n",
      "INFO 05-12 00:54:49 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 05-12 00:54:57 [monitor.py:33] torch.compile takes 10.70 s in total\n",
      "INFO 05-12 00:54:59 [kv_cache_utils.py:634] GPU KV cache size: 50,320 tokens\n",
      "INFO 05-12 00:54:59 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 12.29x\n",
      "INFO 05-12 00:55:32 [gpu_model_runner.py:1626] Graph capturing finished in 32 secs, took 0.51 GiB\n",
      "INFO 05-12 00:55:32 [core.py:163] init engine (profile, create kv cache, warmup model) took 54.21 seconds\n",
      "INFO 05-12 00:55:32 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "mistral = \"/home/yuri011228/ai2-server/models/mistral-7b\"\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=mistral,\n",
    "    tensor_parallel_size=1, # GPU 개수\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_num_seqs = 100, # 동시에 받을 수 있는 요청 개수\n",
    "    max_model_len=4096, # input + output 토큰 길이\n",
    "    max_num_batched_tokens=8192) \n",
    "\n",
    "llm = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 확인용\n",
    "# os.environ[\"LANGSMITH_TRACING\"]  \n",
    "# print(os.getenv(\"QDRANT_HOST\"))\n",
    "# print(os.getenv(\"QDRANT_PORT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29ee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = {\n",
    "  \"email\": \"ConconDev\",\n",
    "  \"date\": \"2024-09-06\",\n",
    "  \"level\": 1,\n",
    "  \"title\": \"알람 구독 서비스 개선: SseEmitter 활용 및 사용자별 Emitter 관리\",\n",
    "  \"keywords\": [\n",
    "    \"SSE\",\n",
    "    \"SseEmitter\",\n",
    "    \"알람 구독\",\n",
    "    \"사용자별 관리\"\n",
    "  ],\n",
    "  \"til\": \"# 알람 구독 서비스 개선: SseEmitter 활용 및 사용자별 Emitter 관리\\n\\n## 1. 오늘 배운 내용\\n\\n오늘 저는 알람 구독 서비스를 개선하기 위해 `SseEmitter`를 활용하고, 각 사용자별로 `SseEmitter`를 저장하고 관리하는 기능을 구현했습니다.  `SseEmitter`는 서버에서 클라이언트로 실시간 데이터를 스트리밍하는 데 유용한 API입니다.\\n\\n## 2. 개념 정리\\n\\n*   **SSE (Server-Sent Events):** 서버에서 클라이언트로 단방향 통신을 가능하게 하는 웹 기술입니다. 서버가 새로운 이벤트 발생 시 클라이언트에게 자동으로 업데이트를 전송합니다.\\n*   **SseEmitter:** SSE 통신을 위한 객체입니다.  데이터를 발행하거나, 연결을 종료하거나, 오류를 처리하는 등의 기능을 제공합니다.\\n*   **ConcurrentHashMap:** 여러 스레드에서 동시에 접근해도 안전하게 데이터를 저장하고 검색할 수 있는 해시맵입니다.  여기서는 사용자 ID를 키로 하고 `SseEmitter`를 값으로 저장하는 데 사용되었습니다.\\n\\n## 3. 해당 개념이 필요한 이유\\n\\n기존 알람 구독 방식은 클라이언트가 주기적으로 서버에 요청을 보내는 방식으로, 서버 부하가 심하고 효율성이 떨어졌습니다. `SseEmitter`를 사용하면 서버는 새로운 알람이 발생했을 때만 클라이언트에 데이터를 전송하므로, 서버 자원을 절약하고 실시간성을 높일 수 있습니다. 또한, 사용자별로 `SseEmitter`를 관리함으로써, 특정 사용자에 대한 알람만 전송하도록 할 수 있어 더욱 효율적인 알람 구독 시스템을 구축할 수 있습니다.\\n\\n## 4. 개념을 활용하는 방법\\n\\n1.  `SseEmitter` 객체를 생성하고 초기화합니다.\\n2.  사용자 ID를 키로, `SseEmitter` 객체를 값으로 `ConcurrentHashMap`에 저장합니다.\\n3.  클라이언트에서 SSE 연결을 설정하고, 서버로부터 데이터를 수신합니다.\\n4.  사용자가 알람 구독/취소 요청을 하면, 해당 사용자의 `SseEmitter`를 업데이트하거나 삭제합니다.\\n\\n## 5. 문제 해결 과정\\n\\n*   처음에는 `SseEmitter`의 생명주기를 제대로 관리하지 못하여 메모리 누수가 발생했습니다.  `SseEmitter` 객체의 `close()` 메서드를 호출하여 리소스 누수를 방지했습니다.\\n*   사용자 ID 중복 문제를 해결하기 위해 사용자 ID를 키로 사용하는 `ConcurrentHashMap`을 사용했습니다.\\n*   클라이언트에서 SSE 연결을 안정적으로 유지하기 위해 에러 핸들링 로직을 추가했습니다.\\n\\n## 6. 하루 회고\\n\\n오늘은 알람 구독 서비스의 성능 개선을 위해 중요한 기술인 `SseEmitter`를 익히고 적용하는 시간을 가졌습니다.  `SseEmitter`의 동작 원리를 이해하고, 실제 서비스에 적용하면서 많은 어려움을 겪었지만, 결국 성공적으로 구현할 수 있었습니다. 앞으로는 `SseEmitter`를 더 깊이 이해하고, 다양한 응용 분야에 적용해보고 싶습니다.\\n\\n## 7. 전체적으로 개조식 문장 구성\\n\\n*   **목표:** 알람 구독 서비스의 실시간성 및 효율성 향상\\n*   **핵심 기술:** SSE, SseEmitter, ConcurrentHashMap\\n*   **구현 단계:**\\n    *   `SseEmitter` 객체 생성 및 초기화\\n    *   사용자별 `SseEmitter` 저장 및 관리 (`ConcurrentHashMap`) \\n    *   SSE 연결 설정 및 데이터 수신\\n    *   알람 구독/취소 요청 처리\\n    *   메모리 누수 방지 및 에러 핸들링\\n\\n\"\n",
    "}\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "당신은 사용자의 기술 학습 기록을 바탕으로, 기술 면접 질문을 생성하는 AI입니다.\n",
    "\n",
    "조건\n",
    "- 레벨: {level}\n",
    "- 사용자 TIL: {til}\n",
    "\n",
    "아래 정보를 참고하여,\n",
    "[검색 결과, 참고 문서] {retrieved}\n",
    "\n",
    "※ level에 따라 질문 수준을 조절해서 면접 질문을 작성해주세요:\n",
    "- level \"1\": 깊은 기술 이해와 실무 경험 기반 질문\n",
    "- level \"2\": 개념적 이해를 묻는 질문\n",
    "- level \"3\": 기본 개념을 묻는 질문\n",
    "\n",
    "반드시 하나의 질문만 만들고 **한국어**로 작성하세요.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd3a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 연습 \n",
    "# from langgraph.graph import StateGraph\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Optional\n",
    "\n",
    "# class ContentState(BaseModel):\n",
    "#     question: str\n",
    "#     answer: str \n",
    "\n",
    "# class QAState(BaseModel):\n",
    "#     email: str\n",
    "#     date: str\n",
    "#     level: int\n",
    "#     title: str\n",
    "#     keywords: List[str]\n",
    "#     til: str\n",
    "\n",
    "#     qa_0: Optional[ContentState] = None\n",
    "#     qa_1: Optional[ContentState] = None\n",
    "#     qa_2: Optional[ContentState] = None\n",
    "#     summary: Optional[str] = None\n",
    "#     content: Optional[List[ContentState]] = None\n",
    "\n",
    "# # dummy retriever\n",
    "# async def retriever_node(state: QAState) -> dict:\n",
    "#     return {}\n",
    "\n",
    "# # 질문 노드\n",
    "# async def question0_node(state: QAState): return {}\n",
    "# async def question1_node(state: QAState): return {}\n",
    "# async def question2_node(state: QAState): return {}\n",
    "\n",
    "# # 답변 노드\n",
    "# async def answer0_node(state: QAState): return {\"qa_0\": {\"question\": \"Q0\", \"answer\": \"A0\"}}\n",
    "# async def answer1_node(state: QAState): return {\"qa_1\": {\"question\": \"Q1\", \"answer\": \"A1\"}}\n",
    "# async def answer2_node(state: QAState): return {\"qa_2\": {\"question\": \"Q2\", \"answer\": \"A2\"}}\n",
    "\n",
    "# # 집계 노드: 세 답변이 모두 도달할 때까지 기다림\n",
    "# async def join_answers_node(state: QAState) -> dict:\n",
    "#     return {}\n",
    "\n",
    "# # 요약 노드\n",
    "# async def summary_node(state: QAState): return {\"summary\": \"요약\", \"content\": []}\n",
    "\n",
    "# # 그래프 구성\n",
    "# workflow = StateGraph(QAState)\n",
    "\n",
    "# # 노드 정의\n",
    "# workflow.add_node(\"retriever\", retriever_node)\n",
    "# workflow.add_node(\"question0\", question0_node)\n",
    "# workflow.add_node(\"question1\", question1_node)\n",
    "# workflow.add_node(\"question2\", question2_node)\n",
    "# workflow.add_node(\"answer0\", answer0_node)\n",
    "# workflow.add_node(\"answer1\", answer1_node)\n",
    "# workflow.add_node(\"answer2\", answer2_node)\n",
    "# workflow.add_node(\"join_answers\", join_answers_node)\n",
    "# workflow.add_node(\"summary_node\", summary_node)\n",
    "\n",
    "\n",
    "\n",
    "# workflow.add_edge(\"join_answers\", \"summary_node\")\n",
    "# workflow.set_finish_point(\"summary_node\")\n",
    "\n",
    "# graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a71449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class ContentState(BaseModel):\n",
    "    question: str\n",
    "    answer: str \n",
    "\n",
    "class QAState(BaseModel):\n",
    "    email: str\n",
    "    date: str\n",
    "    level: int\n",
    "    title: str\n",
    "    keywords: List[str]\n",
    "    til: str\n",
    "\n",
    "    retrieved_texts: Optional[List[str]] = None\n",
    "    similarity_score: Optional[float] = None\n",
    "\n",
    "    question: Optional[str] = None\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "    #output\n",
    "    summary: Optional[str] = None\n",
    "    content: Optional[List[ContentState]] = None\n",
    "\n",
    "qa_input = QAState(\n",
    "    email=dummy['email'],\n",
    "    date=dummy['date'],\n",
    "    level=dummy['level'],\n",
    "    title=dummy['title'],\n",
    "    keywords=dummy['keywords'],\n",
    "    til=dummy['til']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3f43b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from uuid import uuid4\n",
    "from vllm import SamplingParams\n",
    "import os\n",
    "\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "def embed_text(text: str) -> list[float]:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "async def retriever_node(state: QAState) -> dict:\n",
    "    query = state.title + \" \" + \" \".join(state.keywords)\n",
    "    query_vector = embed_text(query)\n",
    "\n",
    "    collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "    best_score = 0.0\n",
    "    retrieved_texts: List[str] = []\n",
    "\n",
    "    for col in collection_names:\n",
    "        results = qdrant.search(\n",
    "            collection_name=col,\n",
    "            query_vector=query_vector,\n",
    "            limit=3,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        if results and results[0].score > best_score:\n",
    "            best_score = results[0].score\n",
    "            retrieved_texts = [h.payload[\"text\"] for h in results if \"text\" in h.payload]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"similarity_score\": best_score,\n",
    "        \"retrieved_texts\": retrieved_texts\n",
    "    }\n",
    "\n",
    "prom = \"\"\"\n",
    "당신은 사용자의 기술 학습 기록을 바탕으로, 기술 면접 질문을 생성하는 AI입니다.\n",
    "\n",
    "아래 정보를 참고하여,\n",
    "[TIL 본문] {til}\n",
    "[RAG 검색 결과] {text}\n",
    "[선택한 난이도] {level}\n",
    "\n",
    "※ level에 따라 질문 수준을 조절해서 면접 질문을 작성해주세요:\n",
    "- level \"1\": 깊은 기술 이해와 실무 경험 기반 질문\n",
    "- level \"2\": 개념적 이해를 묻는 질문\n",
    "- level \"3\": 기본 개념을 묻는 질문\n",
    "\n",
    "모든 질문과 답변은 반드시 **한국어**로 작성하세요.\n",
    "\n",
    "\"\"\"\n",
    "# 질문 생성 노드\n",
    "async def question0_node(state: QAState) -> Dict[str, Any]:\n",
    "    text = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "    prompt = prom.format(\n",
    "        til=state.til,\n",
    "        text=text,\n",
    "        level=state.level\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(prompt, sampling_params, request_id=request_id):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    # 질문만 추출\n",
    "    match = re.search(r\"질문[:：]\\s*(.+)\", final_text)\n",
    "    question = match.group(1).strip() if match else \"질문 생성 실패\"\n",
    "\n",
    "    return {\n",
    "        \"qa_0\": ContentState(question=question, answer=\"\")\n",
    "    }\n",
    "\n",
    "async def question1_node(state: QAState) -> Dict[str, Any]:\n",
    "    generated_q = f\"Q1: '{state.title}' 관련 두 번째 질문입니다.\"\n",
    "    return {\"qa_1\": ContentState(question=generated_q, answer=\"\")}\n",
    "\n",
    "async def question2_node(state: QAState) -> Dict[str, Any]:\n",
    "    generated_q = f\"Q2: '{state.title}' 관련 세 번째 질문입니다.\"\n",
    "    return {\"qa_2\": ContentState(question=generated_q, answer=\"\")}\n",
    "\n",
    "# 답변 생성 노드\n",
    "async def answer0_node(state: QAState) -> dict:\n",
    "    return {\"qa_0\": {\"question\": state.qa_0.question, \"answer\": \"답변0\"}}\n",
    "\n",
    "\n",
    "async def summary_node(state: QAState) -> dict:\n",
    "    # state.qa_0, qa_1, qa_2에 담긴 질문/답변을 모아서 최종 summary, content 생성\n",
    "    content = [state.qa_0]\n",
    "    summary = \"요약 생성 완료\"\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"content\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5886fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "workflow = StateGraph(QAState)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"retriever\",RunnableLambda(retriever_node).with_config({\"run_name\": \"retriever\"}))\n",
    "\n",
    "workflow.add_node(\"summary_node\",RunnableLambda(summary_node).with_config({\"run_name\": \"summary\"}))\n",
    "\n",
    "# 질문/답변 노드 3세트\n",
    "for i in range(3):\n",
    "    q_name = f\"question{i}\"\n",
    "    a_name = f\"answer{i}\"\n",
    "\n",
    "    workflow.add_node(\n",
    "        q_name,\n",
    "        RunnableLambda(globals()[f\"{q_name}_node\"]).with_config({\"run_name\": q_name})\n",
    "    )\n",
    "    workflow.add_node(\n",
    "        a_name,\n",
    "        RunnableLambda(globals()[f\"{a_name}_node\"]).with_config({\"run_name\": a_name})\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"retriever\", q_name)\n",
    "    workflow.add_edge(q_name, a_name)\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "\n",
    "workflow.add_edge(\"answer0\", \"summary_node\")\n",
    "workflow.add_edge(\"answer1\", \"summary_node\")\n",
    "workflow.add_edge(\"answer2\", \"summary_node\")\n",
    "\n",
    "workflow.set_finish_point(\"summary_node\")\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3570c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:430\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == requests.codes.ok:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/IPython/core/formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langgraph/pregel/__init__.py:634\u001b[39m, in \u001b[36mPregel._repr_mimebundle_\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_mimebundle_\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Mime bundle used by Jupyter to display the graph\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    633\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    635\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph.py:685\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    679\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    680\u001b[39m     curve_style=curve_style,\n\u001b[32m    681\u001b[39m     node_colors=node_colors,\n\u001b[32m    682\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    683\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    684\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:293\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    287\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    288\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    289\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    290\u001b[39m         )\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    301\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:462\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    457\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m             msg = (\n\u001b[32m    459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m             ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# This should not be reached, but just in case\u001b[39;00m\n\u001b[32m    465\u001b[39m msg = (\n\u001b[32m    466\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m ) + error_msg_suffix\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph at 0x7ff53e3d71d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7327878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f17a5eb6450>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프 정의 및 구성\n",
    "from langgraph.graph import END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "workflow = StateGraph(QAState)\n",
    "\n",
    "# 노드 선언\n",
    "workflow.add_node(\"retriever\", RunnableLambda(retriever_node).with_config({\"run_name\": \"retriever\"}))\n",
    "\n",
    "# 질문 생성 노드\n",
    "workflow.add_node(\"question0\", RunnableLambda(question0_node).with_config({\"run_name\": \"question0\"}))\n",
    "workflow.add_node(\"question1\", RunnableLambda(question1_node).with_config({\"run_name\": \"question1\"}))\n",
    "workflow.add_node(\"question2\", RunnableLambda(question2_node).with_config({\"run_name\": \"question2\"}))\n",
    "\n",
    "# 답변 생성 노드\n",
    "workflow.add_node(\"answer0\", RunnableLambda(answer0_node).with_config({\"run_name\": \"answer0\"}))\n",
    "workflow.add_node(\"answer1\", RunnableLambda(answer1_node).with_config({\"run_name\": \"answer1\"}))\n",
    "workflow.add_node(\"answer2\", RunnableLambda(answer2_node).with_config({\"run_name\": \"answer2\"}))\n",
    "\n",
    "# fallback + summary\n",
    "# workflow.add_node(\"fallback_generate\", RunnableLambda(fallback_generate_node).with_config({\"run_name\": \"fallback_generate\"}))\n",
    "workflow.add_node(\"summary_node\", RunnableLambda(summary_node).with_config({\"run_name\": \"summary_node\"}))\n",
    "\n",
    "# 조건부 분기 설정\n",
    "# def route_by_similarity(state: QAState) -> str:\n",
    "#     return \"generate_seq\" if state.similarity_score >= 0.5 else \"fallback_generate\"\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"retriever\",\n",
    "#     route_by_similarity,\n",
    "#     {\n",
    "#         \"generate_seq\": [\"question0\", \"question1\", \"question2\"],\n",
    "#         \"fallback_generate\": \"fallback_generate\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "workflow.add_edge(\"retriever\", \"question0\")\n",
    "workflow.add_edge(\"retriever\", \"question1\")\n",
    "workflow.add_edge(\"retriever\", \"question2\")\n",
    "\n",
    "\n",
    "# 각 질문 생성 → 답변 생성 연결\n",
    "workflow.add_edge(\"question0\", \"answer0\")\n",
    "workflow.add_edge(\"question1\", \"answer1\")\n",
    "workflow.add_edge(\"question2\", \"answer2\")\n",
    "\n",
    "# 각 답변 노드 → summary\n",
    "workflow.add_edge(\"answer0\", \"summary_node\")\n",
    "workflow.add_edge(\"answer1\", \"summary_node\")\n",
    "workflow.add_edge(\"answer2\", \"summary_node\")\n",
    "\n",
    "# fallback도 summary로\n",
    "# workflow.add_edge(\"fallback_generate\", \"summary_node\")\n",
    "\n",
    "# 시작, 종료 지점 설정\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "workflow.set_finish_point(\"summary_node\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc49f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "# result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51f30595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-08 08:49:23 [async_llm.py:228] Added request 04e11ffa-2ba6-4d30-ad82-5622e8a42637.\n"
     ]
    }
   ],
   "source": [
    "# # 그래프 정의 및 구성\n",
    "# from langgraph.graph import END\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# workflow = StateGraph(QAState)\n",
    "\n",
    "# # 노드 선언\n",
    "# workflow.add_node(\"retriever\", \n",
    "#     RunnableLambda(retriever_node).with_config({\"run_name\": \"retriever\"}))\n",
    "# workflow.add_node(\"generation0\", \n",
    "#     RunnableLambda(generation0_node).with_config({\"run_name\": \"generation0\"}))\n",
    "# workflow.add_node(\"generation1\", \n",
    "#     RunnableLambda(generation1_node).with_config({\"run_name\": \"generation1\"}))\n",
    "# workflow.add_node(\"generation2\", \n",
    "#     RunnableLambda(generation2_node).with_config({\"run_name\": \"generation2\"}))\n",
    "# workflow.add_node(\"fallback_generate\", \n",
    "#     RunnableLambda(fallback_generate_node).with_config({\"run_name\": \"fallback_generate\"}))\n",
    "# workflow.add_node(\"summary_node\", \n",
    "#     RunnableLambda(summary_node).with_config({\"run_name\": \"summary_node\"}))\n",
    "\n",
    "# # 시작 지점 설정\n",
    "# workflow.set_entry_point(\"retriever\")\n",
    "\n",
    "# # 조건부 분기\n",
    "# def route_by_similarity(state: QAState) -> str:\n",
    "#     return \"generation\" if state.similarity_score >= 0.50 else \"fallback_generate\"\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"retriever\",\n",
    "#     route_by_similarity,\n",
    "#     {\n",
    "#         \"generation\": [\"generation0\", \"generation1\", \"generation2\"],\n",
    "#         \"fallback_generate\": \"fallback_generate\"\n",
    "#     })\n",
    "\n",
    "# # 병렬 분기: generation / fallback → summary\n",
    "# workflow.add_edge(\"generation0\", \"summary_node\")\n",
    "# workflow.add_edge(\"generation1\", \"summary_node\")\n",
    "# workflow.add_edge(\"generation2\", \"summary_node\")\n",
    "# workflow.add_edge(\"fallback_generate\", \"summary_node\")\n",
    "\n",
    "# # 종료 지점 설정\n",
    "# workflow.set_finish_point(\"summary_node\")\n",
    "\n",
    "# graph = workflow.compile()\n",
    "# result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cd0e0",
   "metadata": {},
   "source": [
    "### 레거시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43299b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity_score': 0.52782375, 'retrieved_texts': ['서버 전송 이벤트(Server-sent events, SSE)는 클라이언트가 HTTP 연결을 통해 서버로부터 자동 업데이트를 수신할 수 있도록 하는 서버 푸시 기술이며, 초기 클라이언트 연결이 설정된 후 서버가 클라이언트를 향한 데이터 전송을 시작하는 방법을 설명한다. 이는 일반적으로 브라우저 클라이언트에 메시지 업데이트 또는 지속적인 데이터 스트림을 보내는 데 사용되며 클라이언트가 이벤트 스트림을 수신하기 위해 특정 URL을 요청하는 EventSource라는 자바스크립트 API를 통해 기본 브라우저 간 스트리밍을 향상시키도록 설계되었다. EventSource API는 WHATWG에 의해 HTML Living Standard의 일부로 표준화되었다. SSE의 미디어 유형은 text/event-stream이다. 파이어폭스 6+, 구글 크롬 6+, 오페라 11.5+, 사파리 5+, 마이크로소프트 엣지 79+ 등 모든 최신 브라우저는 서버에서 전송되는 이벤트를 지원한다.', '푸시 기법 기술은 풀 기법으로 명명된 웹브라우저와 비교할 수 있다. 현재 인터넷에서 사용되고 있는 웹브라우저는 사용할 때에 사용자가 항상 자신이 갖고자 하는 정보를 소유한 서버에게 정보를 요청하며 웹브라우저의 사용과 목적지에 대한 최종결정 역시 사용자가 결정할 수 있다. 반면 푸시 기법은 사용자가 일일이 요청하지 않아도 사용자에게 자동으로 뉴스나 사용자가 원하는 특별한 정보, 예를 들면 증권시장의 주기적인 정보 같은 것을 제공한다. 이 두 가지 기법의 차이점은 정보의 흐름을 누가 통제하느냐 하는 점에 있다고 할 수 있다. 풀 기법 하에서는 사용자 (즉 소비자)들이 정보 취득 및 정보의 접촉을 마음대로 통제할 수 있으나, 푸시 기법 하에서는 정보를 전달하는 쪽에서(즉 광고주)정보의 흐름을 직접 통제할 수 있게 된다.', '푸시 기법의 가장 큰 이점은 역시 정보의 맞춤화(Customization)가 가능하다는 점에 있다. 즉 사용자가 이미 등록되어 있기 때문에 등록된 사용자 정보에 의해 타겟을 정확히 선정할 수 있는 것이다.']}\n",
      "INFO 05-12 01:58:06 [async_llm.py:228] Added request 8d7ae736-f076-493c-9eee-9312dcc24de8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7213/925149334.py:27: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. 당신은 오늘 어떤 기술을 배웠습니까?\n",
      "\n",
      "오늘은 알람 구독 서비스를 개선하기 위해 사용되는 `SseEmitter`와 관련된 기술에 대해 배웠습니다. `SseEmitter`는 서버에서 클라이언트로 실시간 데이터를 스트리밍하는 데 사용되는 WebSocket의 대안입니다.\n",
      "\n",
      "### 2. SSE(Server-Sent Events)로 알람 구독 서비스를 개선하는 이유는 무엇입니까?\n",
      "\n",
      "기존의 알람 구독 방식에서는 클라이언트가 주기적으로 서버에 요청을 보내며, 서버는 해당 요청에 대해 알람 정보를 반환합니다. 하지만 이 방식에는 몇 가지 문제점이 있습니다. 먼저, 서버는 모든 클라이언트의 요청을 처리해야 하므로 서버 부하가 심할 수 있습니다. 또한, 클라이언트는 주기적인 요청을 보내야 하므로 효율성도 떨어집니다. 이를 해결하기 위해, 알람 구독 서비스를 SSE와 함께 구현하여 서버는 알람이 발생할 때만 클라이언트에게 알람 정보를 전송하도록 합니다.\n",
      "\n",
      "### 3. SSE(Server-Sent Events)는 무엇입니까?\n",
      "\n",
      "SSE(Server-Sent Events)는 서버에서 클라이언트로 단방향 통신을 가능하게 하는 Web Technology입니다. 서버가 새로운 이벤트가 발생할 때 클라이언트에게 자동으로 업데이트를 전송합니다. 이는 일반적인 HTTP 프로토콜을 사용합니다. 이를 통해 서버는 클라이언트에 실시간 데이터를 전송할 수 있습니다.\n",
      "\n",
      "### 4. SSE를 구현하는데 사용되는 SseEmitter는 무엇입니까?\n",
      "\n",
      "`SseEmitter`는 SSE를 구현하는 데 사용되는 Java API입니다. 이 API는 데이터를 발행하거나, 연결을 종료하거나, 오류를 처리하는 등의 기능을 제공합니다.\n",
      "\n",
      "### 5. SseEmitter를 사용하여 알람 구독 서비스를 구현하는 방법은 무엇입니까?\n",
      "\n",
      "1. `SseEmitter` 객체를 생성하고 초기화합니다.\n",
      "2. 사용자 ID를 키로, `SseEmitter` 객체를 값으로 `ConcurrentHashMap`에 저장합니다.\n",
      "3. 클라이언트에서 SSE 연결을 설정하고, 서버로부터 데이터를 수신합니다.\n",
      "4. 사용자가 알람 구독/취소 요청을 하면, 해당 사용자의 `SseEmitter`를 업데이트하거나 삭제합니다.\n",
      "5. 메모리 누수 방지 및 에러 핸들링을 위해 `SseEmitter`의 `close()` 메서드를 호출합니다.\n",
      "\n",
      "### 6. SSE를 사용하여 알람 구독 서비스를 구현한 이유는 무엇입니까?\n",
      "\n",
      "SSE를 사용하여 알람 구독 서비스를 구현하는 이유는 다음과 같습니다.\n",
      "\n",
      "1. 서버 부하 감소: 서버는 알람이 발생할 때만 클라이언트에게 알람 정보를 전송하므로, 서버 부하가 줄어듭니다.\n",
      "2. 실시간성 높이: 클라이언트는 서버로부터 정보를 주기적으로 요청하지 않아도 되므로, 클라이언트 입장에서는 실시간성이 높아집니다.\n",
      "3. 효율성 높이: 서버는 클라이언트의 요청 대신에 알람이 발생할 때 클라이언트에게 직접 알람 정보를 전송하므로, 전체적인 시스템의 효율성이 높아집니다.\n",
      "\n",
      "### 7. ConcurrentHashMap은 무엇입니까?\n",
      "\n",
      "`ConcurrentHashMap`은 여러 스레드에서 동시에 접근해도 안전하게 데이터를 저장하고 검색할 수 있는 해시맵입니다. 해시맵은 키와 값의 쌍으로 구성된 데이터 구조입니다. `ConcurrentHashMap`은 `SseEmitter`를 관리하기 위해 사용됩니다.\n",
      "\n",
      "### 8. SSE와 WebSocket의 차이는 무엇입니까?\n",
      "\n",
      "SSE(Server-Sent Events)와 WebSocket은 다음과 같은 차이점이 있습니다.\n",
      "\n",
      "1. 통신 방향: SSE는 서버에서 클라이언트로의 단방향 통신을 지원하는 반면, WebSocket은 양방향 통신을 지원합니다.\n",
      "2. 데이터 전송 방식: SSE는 파트IAL HTTP 응답으로 데이터를 전송하고, WebSocket은 전용 프로토콜을 사용합니다.\n",
      "3. 데이터 양: SSE는 일회성으로 데이터를 전송하고, WebSocket은 지속적으로 데이터를 전송할 수 있습니다.\n",
      "\n",
      "### 9. SSE를 사용하여 알람 구독 서비스를 구현하는 장점은 무엇입니까?\n",
      "\n",
      "SSE를 사용하여 알람 구독 서비스를 구현하는 장점은 다음과 같습니다.\n",
      "\n",
      "1. 서버 부하 감소: 서버는 알람이 발생할 때만 클라이언트에게 알람 정보를 전송하므로, 서버 부하가 줄어듭니다.\n",
      "2. 실시간성 높이: 클라이언트는 서버로부터 정보를 주기적으로 요청하지 않아도 되므로, 클라이언트 입장에서는 실시간성이 높아집니다.\n",
      "3. 효율성 높이: 서버는 클라이언트의 요청 대신에 알람이 발생할 때 클라이언트에게 직접 알람 정보를 전송하므로, 전체적인 시스템의 효율성이 높아집니다.\n",
      "4. 간단한 구현: SSE를 사용하여 알람 구독\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langgraph.graph import END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from vllm import SamplingParams\n",
    "from uuid import uuid4\n",
    "from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "def embed_text(text: str) -> list[float]:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "async def retriever_node(state: QAState) -> dict:\n",
    "    query = state.title + \" \" + \" \".join(state.keywords)\n",
    "    query_vector = embed_text(query)\n",
    "\n",
    "    collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "    best_score = 0.0\n",
    "\n",
    "    for col in collection_names:\n",
    "        results = qdrant.search(\n",
    "            collection_name=col,\n",
    "            query_vector=query_vector,\n",
    "            limit=3,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        if results and results[0].score > best_score:\n",
    "            best_score = results[0].score\n",
    "            retrieved_texts = [h.payload[\"text\"] for h in results if \"text\" in h.payload]\n",
    "\n",
    "    return {\n",
    "        \"similarity_score\": best_score,\n",
    "        \"retrieved_texts\": retrieved_texts\n",
    "    }\n",
    "\n",
    "result = await retriever_node(qa_input)\n",
    "print(result)\n",
    "\n",
    "async def generate_node(state: QAState) -> dict:\n",
    "\n",
    "    retrieved = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "\n",
    "    prompt = prompt1.format(\n",
    "        til=state.til,\n",
    "        level=state.level,\n",
    "        retrieved=retrieved\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=2048,\n",
    "        stop_token_ids=[2], \n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"answer_raw\": final_text\n",
    "    }\n",
    "\n",
    "final = await generate_node(qa_input)\n",
    "print(final[\"answer_raw\"])\n",
    "\n",
    "# def route_by_similarity(state: QAState) -> str:\n",
    "#     return \"generation\"\n",
    "\n",
    "# async def generation_node(state: QAState) -> dict:\n",
    "#     return {\"log\": \"generation 노드 실행됨\"}\n",
    "\n",
    "# async def summary_node(state: QAState) -> dict:\n",
    "#     return {\"log_summary\": \"요약 생성 완료\"}\n",
    "\n",
    "# async def final_node(state: QAState) -> dict:\n",
    "#     return {\"log_final\": \"최종 결과 병합 완료\"}\n",
    "\n",
    "# workflow = StateGraph(QAState)\n",
    "\n",
    "# # 노드 선언\n",
    "# workflow.add_node(\"retriever\", \n",
    "#     RunnableLambda(retriever_node).with_config({\n",
    "#         \"run_name\": \"retriever\"}))\n",
    "# workflow.add_node(\"generate\", \n",
    "#     RunnableLambda(generate_node).with_config({\n",
    "#         \"run_name\": \"generate\"}))\n",
    "# workflow.add_node(\"summary_node\", \n",
    "#     RunnableLambda(summary_node).with_config({\n",
    "#         \"run_name\": \"summary_node\"}))\n",
    "\n",
    "# # 시작 지점 설정\n",
    "# workflow.set_entry_point(\"retriever\")\n",
    "\n",
    "# workflow.add_edge(\"retriever\", \"generate\")\n",
    "# workflow.add_edge(\"generate\", \"summary_node\")\n",
    "\n",
    "# # 종료 지점 설정\n",
    "# workflow.set_finish_point(\"summary_node\")\n",
    "\n",
    "# graph = workflow.compile()\n",
    "# result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b1b8b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 생성 → 파싱 테스트 흐름\n",
    "# result = await fallback_generate_node(qa_input)\n",
    "\n",
    "# print(\"LLM 생성된 answer_raw:\")\n",
    "# print(result[\"answer_raw\"])\n",
    "# print(\"\\n---------------------------\\n\")\n",
    "\n",
    "# # QAState에 넣어서 파싱\n",
    "# qa_input.answer_raw = result[\"answer_raw\"]\n",
    "# parsed = await parsing_node(qa_input)\n",
    "\n",
    "# print(\"파싱된 결과:\")\n",
    "# for item in parsed[\"content\"]:\n",
    "#     print(\"질문\", item.question)\n",
    "#     print(\"답변\", item.answer, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이건 입력에 대한 질문입니다: LangGraph 연동 테스트'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from langsmith import traceable\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# @traceable(name=\"generate-question\")\n",
    "# def generate_question(input_text: str) -> str:\n",
    "#     return f\"이건 입력에 대한 질문입니다: {input_text}\"\n",
    "\n",
    "# generate_question(\"LangGraph 연동 테스트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26538afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
