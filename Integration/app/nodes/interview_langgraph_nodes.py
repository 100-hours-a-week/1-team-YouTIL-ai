from tenacity import retry, stop_after_attempt, wait_fixed
from langgraph.graph import StateGraph
from langsmith import traceable
from app.schemas.Interview_Schema import QAState, ContentState
from app.models.interview_model import model
import logging
import re

logger = logging.getLogger(__name__)

class QAFlow:
    def __init__(self, llm, qdrant, templates):
        self.llm = llm
        self.qdrant = qdrant
        self.templates = templates
        self.embedding_model = model.embedder

    def embed_text(self, text: str) -> list[float]:
        return self.embedding_model.encode(text).tolist()
    
    def clean_korean_question(self, text: str) -> str:

        text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)

        text = re.sub(r'\*\*?(Question|Answer|Note|Level).*?\*\*?', '', text, flags=re.IGNORECASE)
        text = re.sub(r'(Question|Answer|Level)\s*[:ï¼š]*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^#+\s*', '', text)

        text = re.sub(r'^[-â€¢\s]+\d*\s*', '', text)

        text = text.replace("`", "").replace("â€œ", "").replace("â€", "")
        text = text.replace("ğŸ‘‰", "").replace("â†’", "").strip()
        text = text.strip().strip('"â€œâ€')

        lines = [line.strip() for line in text.split('\n') if line.strip()]

        return lines[0] if lines else ""

    def generate_question_node(self, node_id: int):
        @traceable(name=f"ì§ˆë¬¸ ìƒì„± ë…¸ë“œ {node_id}", run_type="llm")
        async def question_node(state: QAState) -> dict:
            prompt1 = getattr(self.templates, f"question{node_id}_prompt").format(
                til=state.til,
                level=state.level,
            )

            try:
                final_text = await model.generate_gemini(
                    prompt=prompt1,
                    max_tokens=128,
                    temperature=0.5
                )

                cleaned_question = self.clean_korean_question(final_text)
                return {f"question{node_id}": cleaned_question}
            
            except Exception as e:
                logger.error(f"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
                return {f"question{node_id}": ""}

        return question_node
    
    def generate_retriever_node(self, node_id: int):
        @traceable(name=f"ê²€ìƒ‰ ë…¸ë“œ {node_id}", run_type="retriever")
        async def retriever_node(state: QAState) -> dict:
            question = getattr(state, f"question{node_id}", "")
            query = f"{state.til}\n{question}"
            query_vector = self.embed_text(query)

            results = self.qdrant.search(
                collection_name="tavily_docs",
                query_vector=query_vector,
                limit=1,
                with_payload=True
            )

            retrieved_texts = [r.payload["text"] for r in results if "text" in r.payload]
            best_score = results[0].score if results else 0.0

            return {
                f"similarity_score{node_id}": best_score,
                f"retrieved_texts{node_id}": retrieved_texts
            }
        
        return retriever_node

    def delete_blank(self, text: str) -> str:
        # 1. ìˆ˜í‰ì„  "---" ì œê±° (ì¤„ ë‹¨ë…ì´ê±°ë‚˜ ì•ë’¤ ê°œí–‰ í¬í•¨ëœ ê²½ìš°)
        text = re.sub(r"(?m)^\s*---+\s*$", "", text)

        # 2. ì½”ë“œë¸”ë¡ ë§ˆí¬ë‹¤ìš´ ì œê±°: ``` ë˜ëŠ” ```markdown
        text = re.sub(r"```(?:markdown)?", "", text)

        # 3. \nê³¼ í—¤ë” ì‚¬ì´ê°€ ë¶™ì–´ ìˆì„ ê²½ìš° \n\nìœ¼ë¡œ ë³´ì •
        text = re.sub(r"(?<!\n)\n(###)", r"\n\n\1", text)

        # 4. í—¤ë” ì•„ë‹Œ ì¤„ì€ ë“¤ì—¬ì“°ê¸° ì œê±°
        lines = text.splitlines()
        cleaned_lines = [
            line if line.startswith("###") or line.strip() == "" else line.lstrip()
            for line in lines
        ]

        # 5. ë¶ˆí•„ìš”í•œ ì—°ì† ê°œí–‰ ì œê±° (ìµœëŒ€ 2ì¤„ê¹Œì§€ë§Œ í—ˆìš©)
        cleaned_text = "\n".join(cleaned_lines)
        cleaned_text = re.sub(r"\n{3,}", "\n\n", cleaned_text)

        return cleaned_text.strip()

    def generate_answer_node(self, node_id: int):
        @traceable(name=f"ë‹µë³€ ìƒì„± ë…¸ë“œ {node_id}", run_type="llm")
        async def answer_node(state: QAState) -> dict:
            question = getattr(state, f"question{node_id}", "")
            if not question:
                    logger.warning(f"âš ï¸ ì§ˆë¬¸ {node_id}ê°€ ë¹„ì–´ ìˆìŒ â†’ fallback ì²˜ë¦¬")
                    return {
                        f"content{node_id}": ContentState(
                            question="[ì§ˆë¬¸ ì—†ìŒ]",
                            answer="[ë‹µë³€ ìƒì„± ì‹¤íŒ¨]"
                        )
                    }

            context = getattr(state, f"retrieved_texts{node_id}", None)
            context = "\n\n".join(context) if context else ""

            prompt2 = getattr(self.templates, f"answer{node_id}_prompt").format(
                question=question,
                til=state.til,
                level=state.level,
                context=context
            )

            try:
                final_text = await model.generate_gemini(
                    prompt=prompt2,
                    max_tokens=512,
                    temperature=0.3,
                )

                final_text = self.delete_blank(final_text)
                
                return {
                    f"content{node_id}":ContentState(
                        question=question,
                        answer=final_text
                    )
                }
            
            except Exception as e:
                logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
                return {
                    f"content{node_id}":ContentState(
                        question=question,
                        answer="ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
                    )
                }

        return answer_node

    def is_invalid_summary(self, text: str) -> bool:
        text = text.strip()
        if not text or len(text)>30:
            return True
        if text in ["###", "Q:", "ì œëª© ì—†ìŒ"]:
            return True
        if text.lower().startswith("q:") or text.lower().startswith("a:"):
            return True
        if text in ["###", "ì œëª©:", "ì œëª© ì—†ìŒ"]:
            return True
        return False

    @traceable(name="ìš”ì•½ ìƒì„± ë…¸ë“œ", run_type="llm")
    async def summary_node(self, state: QAState) -> dict:
        merged = []
        for i in range(3):
            item = getattr(state, f"content{i}", None)
            if item:
                merged.append(item)

        qacombined = "\n".join(
            f"Q: {item.question}\nA: {item.answer}" for item in merged
        )

        prompt3 = self.templates.summary.format(
            qacombined = qacombined
        )

        last_valid = None

        for attempt in range(3):
            try:
                final_text = await model.generate_gemini(
                    prompt=prompt3,
                    max_tokens=32,
                    temperature=0.3
                )

                final_text = self.delete_blank(final_text).strip()

                if not final_text or final_text in ["###", "[ìš”ì•½ ì‹¤íŒ¨]"]:
                    logger.warning(f"ìš”ì•½ ì‹œë„ {attempt+1} ì‹¤íŒ¨í•œ ì¶œë ¥: {final_text}")
                    continue

                if not self.is_invalid_summary(final_text):
                    return {
                        "summary": final_text,
                        "content": merged
                    }

                last_valid = final_text

                return {
                    "summary": final_text,
                    "content": merged
                }
            except Exception as e:
                logger.warning(f"ìš”ì•½ ì‹œë„ {attempt+1} ì‹¤íŒ¨: {e}")
                continue
        
        logger.error("ìš”ì•½ ìƒì„± 3íšŒ ì‹¤íŒ¨")
        return {
            "summary": last_valid or "[ìš”ì•½ ì‹¤íŒ¨]",
            "content": merged
        }
        

    def build_graph(self):
        workflow = StateGraph(QAState)

        async def start_node(state: QAState) -> dict:
            return {}
        
        workflow.add_node("start", start_node)
        workflow.set_entry_point("start")

        for i in range(3):
            workflow.add_node(f"que{i}", self.generate_question_node(i))
            workflow.add_node(f"retriever{i}", self.generate_retriever_node(i))
            workflow.add_node(f"ans{i}", self.generate_answer_node(i))

            workflow.add_edge("start", f"que{i}")
            workflow.add_edge(f"que{i}", f"retriever{i}")
            workflow.add_edge(f"retriever{i}", f"ans{i}")
            workflow.add_edge(f"ans{i}", "summary_generate")

        workflow.add_node("summary_generate", self.summary_node)
        workflow.set_finish_point("summary_generate")

        return workflow.compile()